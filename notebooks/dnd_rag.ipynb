{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz33G3t6gbOl"
   },
   "source": [
    "# RAG with Evaluation: Film Box Office ROI Calculator\n",
    "\n",
    "This notebook demonstrates an **agentic RAG system** that analyzes film financial performance by:\n",
    "- Extracting production budgets and box office grosses from Wikipedia\n",
    "- Calculating Return on Investment (ROI)\n",
    "- Classifying film performance (Blockbuster, Profitable, Break-even, Flop)\n",
    "- **Evaluating** against ground truth data\n",
    "\n",
    "This goes beyond simple question-answering‚Äîit requires **multi-step reasoning, calculations, and decision-making**.\n",
    "\n",
    "## Why This Use Case?\n",
    "\n",
    "Unlike simple Q&A, this agent must:\n",
    "1. **Extract** budget and gross from unstructured text\n",
    "2. **Calculate** ROI = (gross - budget) / budget √ó 100\n",
    "3. **Classify** performance based on thresholds\n",
    "4. **Verify** calculations against ground truth\n",
    "\n",
    "At Cohere, all RAG calls come with... **precise citations**! üéâ\n",
    "The model cites which groups of words, in the RAG chunks, were used to generate the final answer.  \n",
    "These citations make it easy to check where the model's generated response claims are coming from.\n",
    "\n",
    "**Note:** The baseline agent and eval harness are intentionally minimal to demonstrate systematic improvement through the eval-driven development cycle.\n",
    "\n",
    "RAG consists of 3 steps:\n",
    "- Step 1: Index film Wikipedia pages and retrieve relevant chunks\n",
    "- Step 2: Optionally, rerank the retrieved chunks\n",
    "- Step 3: Generate financial analysis with **precise citations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSB0pnt0gbOo"
   },
   "source": [
    "## Step 0 - Imports & Getting Film Data\n",
    "\n",
    "In this example, we'll use Wikipedia pages for multiple high-grossing films.   \n",
    "\n",
    "We'll fetch individual film pages (e.g., \"Avatar (2009 film)\", \"Avengers: Endgame\") which contain:\n",
    "- Production budgets\n",
    "- Box office performance\n",
    "- Production details\n",
    "- Release information\n",
    "\n",
    "The agent will need to extract this information and perform calculations independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rACbepFGgbOo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded API key from .env file\n",
      "‚úÖ Cohere client initialized\n"
     ]
    }
   ],
   "source": [
    "# pip install cohere\n",
    "\n",
    "import cohere\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import httpx\n",
    "\n",
    "# Create an insecure HTTP client\n",
    "insecure_client = httpx.Client(verify=False)\n",
    "# Initialize Cohere client\n",
    "# Try to load from .env file first\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('../.env')\n",
    "    print('‚úÖ Loaded API key from .env file')\n",
    "except ImportError:\n",
    "    print('‚ö†Ô∏è  python-dotenv not installed')\n",
    "\n",
    "api_key = os.environ.get('COHERE_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    api_key = input(\"Enter your Cohere API key: \")\n",
    "    os.environ['COHERE_API_KEY'] = api_key\n",
    "\n",
    "co = cohere.ClientV2(api_key=api_key,httpx_client=insecure_client)\n",
    "print('‚úÖ Cohere client initialized')\n",
    "# Get your free API key: https://dashboard.cohere.com/api-keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUph1JX41665",
    "outputId": "6c63a93f-6999-47af-e704-d4a88727bc75"
   },
   "outputs": [],
   "source": [
    "# For chunking let's use langchain to help us split the text\n",
    "# ! pip install -qU langchain-text-splitters -qq\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357874\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/text/spells5e.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhXW7iHC1-Q6",
    "outputId": "d68ac348-4b73-4c6a-a445-6c510bdb0881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has been broken down in 941 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Create basic configurations to chunk the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Split the text into chunks with some overlap\n",
    "chunks_ = text_splitter.create_documents([text])\n",
    "chunks = [c.page_content for c in chunks_]\n",
    "print(f\"The text has been broken down in {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8g0sE2hgbOs"
   },
   "source": [
    "### Embed every text chunk\n",
    "\n",
    "Cohere embeddings are state-of-the-art.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEarMPEqgbOs",
    "outputId": "7da0e06d-f637-4470-8e01-6de8249be64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We just computed 941 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Because the texts being embedded are the chunks we are searching over, we set the input type as search_doc\n",
    "\n",
    "model = \"embed-v4.0\"\n",
    "\n",
    "def batch_embed(texts, batch_size=96):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        response = co.embed(\n",
    "            texts=batch,\n",
    "            model=model,\n",
    "            input_type=\"search_document\",\n",
    "            embedding_types=['float']\n",
    "        )\n",
    "        all_embeddings.extend(response.embeddings.float)\n",
    "    return all_embeddings\n",
    "\n",
    "embeddings = batch_embed(chunks)\n",
    "print(f\"We just computed {len(embeddings)} embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM6vKeypgbOs"
   },
   "source": [
    "### Store the embeddings in a vector database\n",
    "\n",
    "We use the simplest vector database ever: a python dictionary using `np.array()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "sdW7M8HLvB-9"
   },
   "outputs": [],
   "source": [
    "# We use the simplest vector database ever: a python dictionary\n",
    "! pip install numpy -qq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "H2srFH-IgbOs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vector_database = {i: np.array(embedding) for i, embedding in enumerate(embeddings)}\n",
    "# { 0: array([...]), 1: array([...]), 2: array([...]), ..., 10: array([...]) }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6NGVurZgbOs"
   },
   "source": [
    "## Given a user query, retrieve the relevant chunks from the vector database\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC05yJQ7jlek"
   },
   "source": [
    "### Define the user question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Y2HTxspKgbOs"
   },
   "outputs": [],
   "source": [
    "query = \"What level is fireball\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oULg1tOjjOW"
   },
   "source": [
    "### Embed the user question\n",
    "\n",
    "Cohere embeddings are state-of-the-art.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrUuS6vXgbOs",
    "outputId": "0c64a930-f817-43c2-d775-1d9145cb304e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding:  [-0.008171757, 0.011016739, 0.034139782, -0.032202773, -0.055931136, 0.03317128, -0.01779627, 0.0101692965, -0.00950345, -0.011743117, '...']\n"
     ]
    }
   ],
   "source": [
    "# Because the text being embedded is the search query, we set the input type as search_query\n",
    "response = co.embed(\n",
    "    texts=[query],\n",
    "    model=model,\n",
    "    input_type=\"search_query\",\n",
    "    embedding_types=['float']\n",
    ")\n",
    "query_embedding = response.embeddings.float[0]\n",
    "print(\"query_embedding: \", query_embedding[:10] + [\"...\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K8B87CGgbOt"
   },
   "source": [
    "### Retrieve the most relevant chunks from the vector database\n",
    "\n",
    "We use cosine similarity to find the most similar chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nik3es32gbOt",
    "outputId": "a1c30024-52e1-42c7-8836-a2c590559aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated similarity for 941 chunks (top score: 0.5029)\n",
      "Top 20 chunk indices: [np.int64(370), np.int64(380), np.int64(372), np.int64(378), np.int64(245)] ... (showing first 5)\n",
      "Retrieved 20 chunks. Here are the top 3:\n",
      "== Fireball \n",
      "3rd-level evocation \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: 150 feet \n",
      "\n",
      "Components: V, S, M (a tiny ball of bat \n",
      "guano and sulfur) \n",
      "\n",
      "Duration: Instantaneous \n",
      "\n",
      "A bright streak flashes from your pointing finger to a \n",
      "point you choose within range and then blossoms with \n",
      "a low roar into an explosion of flame. Each creature \n",
      "in a 20-foot-radius sphere centered on that point must \n",
      "make a Dexterity saving throw. A target takes 8d6 fire \n",
      "\n",
      "\n",
      "\n",
      "damage on a failed save, or half as much damage on a \n",
      "successful one.\n",
      "== At Higher Levels. When you cast this spell using a \n",
      "spell slot of 6th level or higher, the fire damage or the \n",
      "radiant damage (your choice) increases by ld6 for each \n",
      "slot level above 5th. \n",
      "\n",
      "Flaming Sphere \n",
      "\n",
      "2nd-Ievel conjuration \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: 60 feet \n",
      "\n",
      "Components: V, S, M (a bit of tallow, a pinch of \n",
      "brimstone, and a dusting of powdered iron) \n",
      "\n",
      "Duration: Concentration, up to 1 minute\n",
      "== You hurl a mote of fire at a creature or object within \n",
      "range. Make a ranged spell attack against the \n",
      "target. On a hit, the target takes IdlO fire damage. A \n",
      "flammable object hit by this spell ignites if it isn't being \n",
      "worn or carried. \n",
      "\n",
      "This spell's damage increases by Id 10 when you reach \n",
      "5th level (2dl0), 11th level (3dl0), and 17th level (4dl0). \n",
      "\n",
      "Fire Shield \n",
      "4th-IeveI evocation \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: Self \n",
      "\n",
      "Components: V. S, M (a bit of phosphorus or a firefly) \n",
      "\n",
      "Duration: 10 minutes\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_response_text(response) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from Cohere response.\n",
    "    Handles both reasoning models (command-a-reasoning-*) and non-reasoning models.\n",
    "    \"\"\"\n",
    "    for item in response.message.content:\n",
    "        if item.type == 'text':\n",
    "            return item.text\n",
    "    return \"\"\n",
    "\n",
    "# Calculate similarity between the user question & each chunk\n",
    "similarities = [cosine_similarity(query_embedding, chunk) for chunk in embeddings]\n",
    "print(f\"Calculated similarity for {len(similarities)} chunks (top score: {max(similarities):.4f})\")\n",
    "\n",
    "# Get indices of the top 20 most similar chunks\n",
    "sorted_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "# Keep only the top 20 indices\n",
    "top_indices = sorted_indices[:20]\n",
    "print(f\"Top 20 chunk indices: {list(top_indices[:5])} ... (showing first 5)\")\n",
    "\n",
    "# Retrieve the top 20 most similar chunks\n",
    "top_chunks_after_retrieval = [chunks[i] for i in top_indices]\n",
    "print(f\"Retrieved {len(top_chunks_after_retrieval)} chunks. Here are the top 3:\")\n",
    "for t in top_chunks_after_retrieval[:3]:\n",
    "    print(\"== \" + t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzcpds3VgbOt"
   },
   "source": [
    "## Step 2 - Rerank the chunks retrieved from the vector database\n",
    "\n",
    "We rerank the 10 chunks retrieved from the vector database. Reranking boosts retrieval accuracy.\n",
    "\n",
    "Reranking lets us go from 10 chunks retrieved from the vector database, to the 3 most relevant chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2J4LywVygbOt",
    "outputId": "7a4c89bf-fc5e-409f-9304-fce006b9d8bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 5 chunks after rerank: \n",
      "== You send negative energy coursing through a creature \n",
      "that you can see within range, causing it searing pain. \n",
      "The target must make a Constitution saving throw. It \n",
      "takes 7d8 + 30 necrotic damage on a failed save, or half \n",
      "as much damage on a successful one. \n",
      "\n",
      "A humanoid killed by this spell rises at the start of \n",
      "your next turn as a zombie that is permanently under \n",
      "your command, following your verbal orders to the best \n",
      "of its ability. \n",
      "\n",
      "Fireball \n",
      "3rd-level evocation\n",
      "== Fireball \n",
      "3rd-level evocation \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: 150 feet \n",
      "\n",
      "Components: V, S, M (a tiny ball of bat \n",
      "guano and sulfur) \n",
      "\n",
      "Duration: Instantaneous \n",
      "\n",
      "A bright streak flashes from your pointing finger to a \n",
      "point you choose within range and then blossoms with \n",
      "a low roar into an explosion of flame. Each creature \n",
      "in a 20-foot-radius sphere centered on that point must \n",
      "make a Dexterity saving throw. A target takes 8d6 fire \n",
      "\n",
      "\n",
      "\n",
      "damage on a failed save, or half as much damage on a \n",
      "successful one.\n",
      "== If the spell is still in effect when the target is subjected \n",
      "to an effect that would kill it instantaneously without \n",
      "dealing damage, that effect is instead negated against \n",
      "the target, and the spell ends. \n",
      "\n",
      "Delayed Blast Fireball \n",
      "\n",
      "7th-Ievel evocation \n",
      "\n",
      "Casting Time: 1 action \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Range: 150 feet \n",
      "\n",
      "Components: V, S, M (a tiny ball of bat \n",
      "guano and sulfur) \n",
      "\n",
      "Duration: Concentration, up to 1 minute\n",
      "== You can also attack with the flame, although doing so \n",
      "ends the spell. When you cast this spell, or as an action \n",
      "on a later turn, you can hurl the flame at a creature \n",
      "within 30 feet of you. Make a ranged spell attack. On a \n",
      "hit, the target takes IdS fire damage. \n",
      "\n",
      "This spell‚Äôs damage increases by IdS when you reach \n",
      "5th level (2d8), 11th level (3d8), and 17th level (4d8). \n",
      "\n",
      "Programmed Illusion \n",
      "6th-level illusion \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: 120 feet\n",
      "== One side of the wall, selected by you when you cast \n",
      "this spell, deals 5d8 fire damage to each creature that \n",
      "ends its turn within 10 feet of that side or inside the \n",
      "wall. A creature takes the same damage when it enters \n",
      "the wall for the first time on a turn or ends its turn there. \n",
      "The other side of the wall deals no damage. \n",
      "\n",
      "At Higher Levels. When you cast this spell using a \n",
      "spell slot of 5th level or higher, the damage increases by \n",
      "ld8 for each slot level above 4th.\n"
     ]
    }
   ],
   "source": [
    "response = co.rerank(\n",
    "    query=query,\n",
    "    documents=top_chunks_after_retrieval,\n",
    "    top_n=5,\n",
    "    model=\"rerank-v3.5\",\n",
    ")\n",
    "\n",
    "# top_chunks_after_rerank = [result.document['text'] for result in response]\n",
    "\n",
    "top_chunks_after_rerank = [top_chunks_after_retrieval[result.index] for result in response.results]\n",
    "\n",
    "print(\"Here are the top 5 chunks after rerank: \")\n",
    "for t in top_chunks_after_rerank:\n",
    "    print(\"== \" + t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuPL0VUXgbOt"
   },
   "source": [
    "## Step 3 - Generate the model final answer, given the retrieved and reranked chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "oCNXWH8GgbOt"
   },
   "outputs": [],
   "source": [
    "# preamble containing instructions about the task and the desired style for the output.\n",
    "preamble = \"\"\"\n",
    "## Task & Context\n",
    "You help people answer their questions about rules of the roleplaying game Dungeons and Dragons. You will be asked a very wide array of requests on all kinds of topics relating to the rules of Dungeons and Dragons. You will have access to a search function over the entire rulesbook. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n",
    "The question does not always contain all the information that is needed to answer the question, in that case ask the user for more information.\n",
    "\n",
    "## Style Guide\n",
    "Please return the minimun amount of information, if the user asks for a number answer only with the number. Otherwise keep the answer short.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BevatShtgbOt",
    "outputId": "af71f4a9-787a-4ee3-9598-20692fb3bf16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing 5 documents to the model\n",
      "Final answer:\n",
      "3rd.\n"
     ]
    }
   ],
   "source": [
    "# retrieved documents - now using all 5 reranked chunks\n",
    "documents = [\n",
    "    {\"data\": {\"title\": f\"chunk {i}\", \"snippet\": chunk}} \n",
    "    for i, chunk in enumerate(top_chunks_after_rerank)\n",
    "]\n",
    "\n",
    "print(f\"Passing {len(documents)} documents to the model\")\n",
    "\n",
    "# get model response\n",
    "response = co.chat(\n",
    "  model=\"command-a-reasoning-08-2025\",\n",
    "  messages=[{\"role\" : \"system\", \"content\" : preamble},\n",
    "            {\"role\" : \"user\", \"content\" : query}],\n",
    "  documents=documents,  \n",
    "  temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"Final answer:\")\n",
    "print(get_response_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20wcn-EjlXZd"
   },
   "source": [
    "Note: this is indeed the answer you'd expect, and here was the passage of text in wikipedia explaining it!\n",
    "\n",
    "\" [...] Star Wars was originally scheduled to be released on October 20, 2023, but was delayed to November 17, 2023, before moving forward two weeks to November 3, 2023, to adjust to changes in release schedules from other studios. It was later postponed by over four months to March 15, 2024, due to the 2023 Hollywood labor disputes. After the strikes were resolved, the film moved once more up two weeks to March 1, 2024. [...]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoSVDXSsgbOt"
   },
   "source": [
    "## Bonus: Citations come for free with Cohere! üéâ\n",
    "\n",
    "At Cohere, all RAG calls come with... precise citations! üéâ\n",
    "The model cites which groups of words, in the RAG chunks, were used to generate the final answer.  \n",
    "These citations make it easy to check where the model‚Äôs generated response claims are coming from.  \n",
    "They help users gain visibility into the model reasoning, and sanity check the final model generation.  \n",
    "These citations are optional ‚Äî you can decide to ignore them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVTuQdmDgbOt",
    "outputId": "f843b262-d8bb-45ba-cbfb-9915da104eda"
   },
   "outputs": [],
   "source": [
    "print(\"Citations that support the final answer:\")\n",
    "for cite in response.message.citations:\n",
    "    print(cite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IueXaIJggbOu",
    "outputId": "c816af51-74be-42c9-e94e-9820bbf95f79"
   },
   "outputs": [],
   "source": [
    "def insert_inline_citations(text, citations, field='text'):\n",
    "    sorted_citations = sorted(citations, key=lambda c: c.start, reverse=True)\n",
    "    \n",
    "    for citation in sorted_citations:\n",
    "        source_ids = [source.id.split(':')[-1] for source in citation.sources]\n",
    "        citation_text = f\"[{','.join(source_ids)}]\"\n",
    "        text = text[:citation.end] + citation_text + text[citation.end:]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def list_sources(citations, fields=['text']):\n",
    "    unique_sources = set()\n",
    "    for citation in citations:\n",
    "        for source in citation.sources:\n",
    "            source_data = tuple((field, source.document[field]) for field in fields if field in source.document)\n",
    "            unique_sources.add((source.id.split(':')[-1], source_data))\n",
    "    \n",
    "    footnotes = []\n",
    "    for source_id, source_data in sorted(unique_sources):\n",
    "        footnote = f\"[{source_id}] \" + \", \".join(f\"{key}: {value}\" for key, value in source_data)\n",
    "        footnotes.append(footnote)\n",
    "    \n",
    "    return \"\\n\".join(footnotes)\n",
    "\n",
    "# Use the functions\n",
    "cited_text = insert_inline_citations(response.message.content[1].text, response.message.citations)\n",
    "\n",
    "# Print the result with inline citations\n",
    "print(cited_text)\n",
    "\n",
    "# Print footnotes\n",
    "if response.message.citations:\n",
    "    print(\"\\nSource documents:\")\n",
    "    print(list_sources(response.message.citations, fields=['title','snippet']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kp4c_HkYIEn_"
   },
   "outputs": [],
   "source": [
    "def insert_inline_citations(text, citations, field='text'):\n",
    "    sorted_citations = sorted(citations, key=lambda c: c.start, reverse=True)\n",
    "    \n",
    "    for citation in sorted_citations:\n",
    "        source_ids = [source.id.split(':')[-1] for source in citation.sources]\n",
    "        citation_text = f\"[{','.join(source_ids)}]\"\n",
    "        text = text[:citation.end] + citation_text + text[citation.end:]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def list_sources(citations):\n",
    "    unique_sources = set()\n",
    "    for citation in citations:\n",
    "        for source in citation.sources:\n",
    "            source_data = tuple((key, value) for key, value in source.document.items() if key != 'id')\n",
    "            unique_sources.add((source.id.split(':')[-1], source_data))\n",
    "    \n",
    "    footnotes = []\n",
    "    for source_id, source_data in sorted(unique_sources):\n",
    "        footnote = f\"[{source_id}] \" + \", \".join(f\"{key}: {value}\" for key, value in source_data)\n",
    "        footnotes.append(footnote)\n",
    "    \n",
    "    return \"\\n\".join(footnotes)\n",
    "\n",
    "# Use the functions\n",
    "cited_text = insert_inline_citations(response.message.content[1].text, response.message.citations)\n",
    "\n",
    "# Print the result with inline citations\n",
    "print(cited_text)\n",
    "\n",
    "# Print footnotes\n",
    "if response.message.citations:\n",
    "    print(\"\\nSource documents:\")\n",
    "    print(list_sources(response.message.citations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: RAG Evaluation\n",
    "\n",
    "Now that we've built a RAG system, let's evaluate its performance!\n",
    "\n",
    "We'll create test queries about Star Wars and measure how well our RAG system answers them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluation Section\n",
    "\n",
    "Now we'll evaluate our agent's ability to extract financial data and calculate ROI correctly.\n",
    "\n",
    "## Define Test Queries\n",
    "\n",
    "We'll create test queries that ask the agent to analyze films and extract their financial performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for Film ROI Analysis\n",
    "# These match the films fetched from Wikipedia and include ground truth values from CSV\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What was Avengers: Endgame's budget and how much did it gross worldwide? What was the ROI?\",\n",
    "        \"film_title\": \"Avengers: Endgame\",\n",
    "        \"year\": 2019,\n",
    "        \"budget\": 356.0,              # Ground truth from CSV (millions)\n",
    "        \"worldwide_gross\": 2797.5,    # Ground truth from CSV (millions)\n",
    "        \"roi\": 685.81,                # Ground truth from CSV (percent)\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How much did Titanic cost to make and how much money did it earn at the box office?\",\n",
    "        \"film_title\": \"Titanic\",\n",
    "        \"year\": 1997,\n",
    "        \"budget\": 200.0,              # Ground truth from CSV (millions)\n",
    "        \"worldwide_gross\": 2257.9,    # Ground truth from CSV (millions)\n",
    "        \"roi\": 1028.95,               # Ground truth from CSV (percent)\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What was the production budget and worldwide box office gross for Star Wars: The Force Awakens?\",\n",
    "        \"film_title\": \"Star Wars: The Force Awakens\",\n",
    "        \"year\": 2015,\n",
    "        \"budget\": 245.0,              # Ground truth from CSV (millions)\n",
    "        \"worldwide_gross\": 2068.2,    # Ground truth from CSV (millions)\n",
    "        \"roi\": 744.17,                # Ground truth from CSV (percent)\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What was the budget and worldwide gross for Avengers: Infinity War?\",\n",
    "        \"film_title\": \"Avengers: Infinity War\",\n",
    "        \"year\": 2018,\n",
    "        \"budget\": 400.0,              # Ground truth from CSV (millions)\n",
    "        \"worldwide_gross\": 2048.4,    # Ground truth from CSV (millions)\n",
    "        \"roi\": 412.09,                # Ground truth from CSV (percent)\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Tell me about Spider-Man: No Way Home's financial performance - budget, gross, and profitability.\",\n",
    "        \"film_title\": \"Spider-Man: No Way Home\",\n",
    "        \"year\": 2021,\n",
    "        \"budget\": 200.0,              # Ground truth from CSV (millions)\n",
    "        \"worldwide_gross\": 1922.6,    # Ground truth from CSV (millions)\n",
    "        \"roi\": 861.30,                # Ground truth from CSV (percent)\n",
    "        \"difficulty\": \"hard\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function\n",
    "\n",
    "This function will:\n",
    "1. Run the RAG pipeline on each query\n",
    "2. Check if expected keywords appear in the answer\n",
    "3. Evaluate citation quality\n",
    "4. Measure response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "import re\n",
    "\n",
    "def parse_financial_numbers(text: str) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Extract budget, gross, and ROI from text response\n",
    "    Returns values in millions of dollars\n",
    "    \"\"\"\n",
    "    result = {\"budget\": None, \"gross\": None, \"roi\": None}\n",
    "    \n",
    "    # Pattern for budget (in millions or billions)\n",
    "    budget_patterns = [\n",
    "        r'budget.*?\\$(\\d+(?:\\.\\d+)?)\\s*billion',\n",
    "        r'budget.*?\\$(\\d+(?:,\\d+)?(?:\\.\\d+)?)\\s*million',\n",
    "        r'\\$(\\d+(?:,\\d+)?(?:\\.\\d+)?)\\s*million.*?budget'\n",
    "    ]\n",
    "    \n",
    "    # Pattern for gross (in millions or billions)\n",
    "    gross_patterns = [\n",
    "        r'(?:gross|earned|made).*?\\$(\\d+(?:\\.\\d+)?)\\s*billion',\n",
    "        r'(?:gross|earned|made).*?\\$(\\d+(?:,\\d+)?(?:\\.\\d+)?)\\s*million',\n",
    "        r'\\$(\\d+(?:\\.\\d+)?)\\s*billion.*?(?:gross|worldwide)'\n",
    "    ]\n",
    "    \n",
    "    # Pattern for ROI percentage\n",
    "    roi_patterns = [\n",
    "        r'ROI.*?(\\d+(?:\\.\\d+)?)\\s*%',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*%.*?ROI',\n",
    "        r'return.*?(\\d+(?:\\.\\d+)?)\\s*%'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Extract budget\n",
    "    for pattern in budget_patterns:\n",
    "        match = re.search(pattern, text_lower, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = float(match.group(1).replace(',', ''))\n",
    "            result[\"budget\"] = value * 1000 if 'billion' in match.group(0).lower() else value\n",
    "            break\n",
    "    \n",
    "    # Extract gross\n",
    "    for pattern in gross_patterns:\n",
    "        match = re.search(pattern, text_lower, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = float(match.group(1).replace(',', ''))\n",
    "            result[\"gross\"] = value * 1000 if 'billion' in match.group(0).lower() else value\n",
    "            break\n",
    "    \n",
    "    # Extract ROI\n",
    "    for pattern in roi_patterns:\n",
    "        match = re.search(pattern, text_lower, re.IGNORECASE)\n",
    "        if match:\n",
    "            result[\"roi\"] = float(match.group(1))\n",
    "            break\n",
    "    \n",
    "    return result\n",
    "\n",
    "def evaluate_response(response: str, expected_budget: float, expected_gross: float, expected_roi: float) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate response by comparing extracted numbers to ground truth\n",
    "    All values should be in millions\n",
    "    \"\"\"\n",
    "    parsed = parse_financial_numbers(response)\n",
    "    \n",
    "    # Calculate errors (allow 5% tolerance)\n",
    "    tolerance = 0.05\n",
    "    \n",
    "    budget_match = False\n",
    "    gross_match = False\n",
    "    roi_match = False\n",
    "    \n",
    "    budget_error = None\n",
    "    gross_error = None\n",
    "    roi_error = None\n",
    "    \n",
    "    if parsed[\"budget\"]:\n",
    "        budget_error = abs(parsed[\"budget\"] - expected_budget) / expected_budget\n",
    "        budget_match = budget_error <= tolerance\n",
    "    \n",
    "    if parsed[\"gross\"]:\n",
    "        gross_error = abs(parsed[\"gross\"] - expected_gross) / expected_gross\n",
    "        gross_match = gross_error <= tolerance\n",
    "    \n",
    "    if parsed[\"roi\"]:\n",
    "        roi_error = abs(parsed[\"roi\"] - expected_roi) / expected_roi\n",
    "        roi_match = roi_error <= tolerance\n",
    "    \n",
    "    # Pass if at least budget and gross are correct (ROI can be calculated from these)\n",
    "    passed = budget_match and gross_match\n",
    "    \n",
    "    return {\n",
    "        \"passed\": passed,\n",
    "        \"budget_match\": budget_match,\n",
    "        \"gross_match\": gross_match,\n",
    "        \"roi_match\": roi_match,\n",
    "        \"parsed_budget\": parsed[\"budget\"],\n",
    "        \"parsed_gross\": parsed[\"gross\"],\n",
    "        \"parsed_roi\": parsed[\"roi\"],\n",
    "        \"budget_error\": budget_error,\n",
    "        \"gross_error\": gross_error,\n",
    "        \"roi_error\": roi_error\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Evaluation function created with ground truth comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Let's test the agent on all our film financial queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, test in enumerate(test_cases):\n",
    "    print(f\"\\nEvaluating query {i+1}/{len(test_cases)}: {test['query'][:60]}...\")\n",
    "    \n",
    "    # Embed and retrieve\n",
    "    query_resp = co.embed(\n",
    "        texts=[test['query']],\n",
    "        model=model,\n",
    "        input_type=\"search_query\",\n",
    "        embedding_types=['float']\n",
    "    )\n",
    "    q_embedding = query_resp.embeddings.float[0]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sims = [cosine_similarity(q_embedding, chunk_emb) for chunk_emb in embeddings]\n",
    "    top_idx = np.argsort(sims)[-20:][::-1]\n",
    "    top_chunks = [chunks[i] for i in top_idx]\n",
    "    \n",
    "    # Rerank\n",
    "    rerank_resp = co.rerank(\n",
    "        query=test['query'],\n",
    "        documents=top_chunks,\n",
    "        top_n=5,\n",
    "        model=\"rerank-v3.5\",\n",
    "    )\n",
    "    reranked = [top_chunks[r.index] for r in rerank_resp.results]\n",
    "    \n",
    "    # Generate response\n",
    "    docs = [{\"data\": {\"snippet\": doc}} for doc in reranked]\n",
    "    chat_resp = co.chat(\n",
    "        model=\"command-a-reasoning-08-2025\",\n",
    "        messages=[{\"role\": \"user\", \"content\": test['query']}],\n",
    "        documents=docs,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    answer = get_response_text(chat_resp)\n",
    "    \n",
    "    # Evaluate against ground truth from CSV\n",
    "    eval_result = evaluate_response(\n",
    "        answer, \n",
    "        test['budget'], \n",
    "        test['worldwide_gross'], \n",
    "        test['roi']\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"query\": test['query'],\n",
    "        \"answer\": answer,\n",
    "        \"film_title\": test['film_title'],\n",
    "        \"difficulty\": test['difficulty'],\n",
    "        **eval_result\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    status = \"‚úÖ PASS\" if eval_result['passed'] else \"‚ùå FAIL\"\n",
    "    budget_status = \"‚úì\" if eval_result['budget_match'] else \"‚úó\"\n",
    "    gross_status = \"‚úì\" if eval_result['gross_match'] else \"‚úó\"\n",
    "    roi_status = \"‚úì\" if eval_result['roi_match'] else \"‚úó\"\n",
    "    print(f\"  {status} | Budget: {budget_status} | Gross: {gross_status} | ROI: {roi_status}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Let's see how well our agent extracted and calculated film financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "total_tests = len(results)\n",
    "passed = sum(1 for r in results if r['passed'])\n",
    "budget_correct = sum(1 for r in results if r['budget_match'])\n",
    "gross_correct = sum(1 for r in results if r['gross_match'])\n",
    "roi_correct = sum(1 for r in results if r['roi_match'])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RAG EVALUATION RESULTS - Film Box Office ROI Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total queries tested: {total_tests}\")\n",
    "print(f\"Overall pass rate: {passed}/{total_tests} ({passed/total_tests*100:.1f}%)\")\n",
    "print()\n",
    "print(f\"Accuracy by metric:\")\n",
    "print(f\"  Budget extraction:  {budget_correct}/{total_tests} ({budget_correct/total_tests*100:.1f}%)\")\n",
    "print(f\"  Gross extraction:   {gross_correct}/{total_tests} ({gross_correct/total_tests*100:.1f}%)\")\n",
    "print(f\"  ROI calculation:    {roi_correct}/{total_tests} ({roi_correct/total_tests*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Show details for failures\n",
    "failures = [r for r in results if not r['passed']]\n",
    "if failures:\n",
    "    print(f\"Failed queries: {len(failures)}\")\n",
    "    for fail in failures:\n",
    "        print(f\"  ‚ùå {fail['film_title']}\")\n",
    "        if fail['parsed_budget']:\n",
    "            print(f\"     Budget: ${fail['parsed_budget']:.1f}M (error: {fail['budget_error']*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"     Budget: Not found\")\n",
    "        if fail['parsed_gross']:\n",
    "            print(f\"     Gross: ${fail['parsed_gross']:.1f}M (error: {fail['gross_error']*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"     Gross: Not found\")\n",
    "else:\n",
    "    print(\"üéâ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Detailed Results\n",
    "\n",
    "Let's examine each query to see how the agent performed on film financial extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test {i}: {result['film_title']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"\\nPassed: {'‚úì' if result['passed'] else '‚úó'}\")\n",
    "    # print(f\"Response time: {result['response_time']:.2f}s\")\n",
    "    # print(f\"Citations: {len(result['citations'])}\")\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{result['answer'][:500]}...\")  # First 500 chars\n",
    "    \n",
    "    print(f\"\\nKeyword check:\")\n",
    "    # for keyword in result['expected_keywords']:\n",
    "    #     found = keyword.lower() in result['answer'].lower()\n",
    "    #     print(f\"  {'‚úì' if found else '‚úó'} '{keyword}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Summary\n",
    "\n",
    "### What We Built:\n",
    "\n",
    "**Part 1: RAG Tutorial**\n",
    "- ‚úÖ Document chunking and embedding with `embed-v4.0`\n",
    "- ‚úÖ Vector similarity search (cosine similarity)\n",
    "- ‚úÖ Reranking with `rerank-v3.5`\n",
    "- ‚úÖ Generation with `command-a-reasoning-08-2025` and **citations**\n",
    "- ‚úÖ Citation formatting utilities\n",
    "\n",
    "**Part 2: Evaluation**\n",
    "- ‚úÖ Test queries on the same dataset\n",
    "- ‚úÖ Automated evaluation with keyword coverage\n",
    "- ‚úÖ Pass/fail analysis by difficulty\n",
    "- ‚úÖ Detailed results inspection\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **RAG improves accuracy** by grounding responses in retrieved documents\n",
    "2. **Citations build trust** by showing which sources were used\n",
    "3. **Reranking boosts performance** by prioritizing the most relevant chunks\n",
    "4. **Evaluation is essential** to measure and improve RAG systems\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Try different parameters**: Experiment with chunk sizes, top-k values, temperature\n",
    "- **Add more test queries**: Create a larger evaluation set\n",
    "- **Improve retrieval**: Try different embedding models or hybrid search\n",
    "- **Enhance evaluation**: Add more sophisticated metrics (BLEU, ROUGE, LLM-as-judge)\n",
    "\n",
    "**Happy building! üöÄ**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Advanced Evaluation: Ground Truth Validation\n",
    "\n",
    "Now let's evaluate the agent's ability to **extract accurate financial data** and **calculate ROI correctly** by comparing against our ground truth dataset.\n",
    "\n",
    "This evaluation will measure:\n",
    "1. **Data Extraction Accuracy**: Did the agent find the correct budget and gross figures?\n",
    "2. **Calculation Correctness**: Is the ROI calculation mathematically accurate?\n",
    "3. **Classification Accuracy**: Does the performance classification match the actual ROI?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv('../data/ground_truth/film_box_office_ground_truth.csv')\n",
    "\n",
    "print(f\"Loaded {len(ground_truth_df)} films from ground truth\")\n",
    "print(\"\\nSample data:\")\n",
    "print(ground_truth_df[['Year', 'Title', 'Budget', 'Worldwide gross', 'ROI']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_financial_data_with_rag(film_title, film_year=None):\n",
    "    \"\"\"\n",
    "    Use RAG to extract budget and gross for a specific film\n",
    "    \"\"\"\n",
    "    # Construct query\n",
    "    query = f\"What was the production budget and worldwide box office gross for {film_title}?\"\n",
    "    if film_year:\n",
    "        query += f\" (from {film_year})\"\n",
    "    \n",
    "    # Embed query\n",
    "    query_embed = co.embed(\n",
    "        texts=[query],\n",
    "        model=model,\n",
    "        input_type=\"search_query\",\n",
    "        embedding_types=[\"float\"]\n",
    "    ).embeddings.float[0]\n",
    "    \n",
    "    # Retrieve chunks\n",
    "    doc_ids = [id for id, _ in sorted(\n",
    "        [(id, cosine_similarity(query_embed, emb)) for id, emb in embeddings.items()],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:20]]\n",
    "    docs_to_use = [chunks[id] for id in doc_ids]\n",
    "    \n",
    "    # Rerank\n",
    "    rerank_results = co.rerank(\n",
    "        query=query,\n",
    "        documents=docs_to_use,\n",
    "        top_n=5,\n",
    "        model=\"rerank-v3.5\"\n",
    "    )\n",
    "    \n",
    "    docs_to_use = [docs_to_use[r.index] for r in rerank_results.results]\n",
    "    \n",
    "    # Generate response with specific instructions\n",
    "    preamble = \"\"\"You are a financial analyst extracting film production data. \n",
    "    Extract the EXACT budget and worldwide box office gross figures from the provided text.\n",
    "    Report numbers in millions (e.g., '$237 million' or '$2.9 billion').\n",
    "    If you find the data, also calculate the ROI using: ROI = (gross - budget) / budget √ó 100\"\"\"\n",
    "    \n",
    "    response = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        documents=[{\"text\": doc} for doc in docs_to_use],\n",
    "        preamble=preamble\n",
    "    )\n",
    "    \n",
    "    return get_response_text(response), response.message.citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_financial_data(agent_response):\n",
    "    \"\"\"\n",
    "    Parse budget and gross from agent's text response\n",
    "    Returns values in dollars (not millions)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Look for budget\n",
    "    budget = None\n",
    "    budget_patterns = [\n",
    "        r'budget.*?\\$([\\d,]+)\\s*million',\n",
    "        r'\\$([\\d,]+)\\s*million.*?budget',\n",
    "        r'cost.*?\\$([\\d,]+)\\s*million',\n",
    "    ]\n",
    "    \n",
    "    for pattern in budget_patterns:\n",
    "        match = re.search(pattern, agent_response, re.IGNORECASE)\n",
    "        if match:\n",
    "            budget = float(match.group(1).replace(',', '')) * 1_000_000\n",
    "            break\n",
    "    \n",
    "    # Look for gross\n",
    "    gross = None\n",
    "    gross_patterns = [\n",
    "        r'gross.*?\\$([\\d.]+)\\s*billion',\n",
    "        r'\\$([\\d.]+)\\s*billion.*?gross',\n",
    "        r'earned.*?\\$([\\d.]+)\\s*billion',\n",
    "        r'box office.*?\\$([\\d.]+)\\s*billion',\n",
    "    ]\n",
    "    \n",
    "    for pattern in gross_patterns:\n",
    "        match = re.search(pattern, agent_response, re.IGNORECASE)\n",
    "        if match:\n",
    "            gross = float(match.group(1)) * 1_000_000_000\n",
    "            break\n",
    "    \n",
    "    # If not found in billions, try millions\n",
    "    if gross is None:\n",
    "        gross_patterns_mil = [\n",
    "            r'gross.*?\\$([\\d,]+)\\s*million',\n",
    "            r'\\$([\\d,]+)\\s*million.*?gross',\n",
    "        ]\n",
    "        for pattern in gross_patterns_mil:\n",
    "            match = re.search(pattern, agent_response, re.IGNORECASE)\n",
    "            if match:\n",
    "                gross = float(match.group(1).replace(',', '')) * 1_000_000\n",
    "                break\n",
    "    \n",
    "    # Calculate ROI if we have both\n",
    "    roi = None\n",
    "    if budget and gross and budget > 0:\n",
    "        roi = ((gross - budget) / budget) * 100\n",
    "    \n",
    "    return {\n",
    "        'budget': budget,\n",
    "        'gross': gross,\n",
    "        'roi': roi\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a subset of films from ground truth (matching Wikipedia dataset)\n",
    "test_films = [\n",
    "    ('Avengers: Endgame', 2019),\n",
    "    ('Titanic', 1997),\n",
    "    ('Star Wars: The Force Awakens', 2015),\n",
    "    ('Avengers: Infinity War', 2018),\n",
    "    ('Spider-Man: No Way Home', 2021),\n",
    "]\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for film_title_short, year in test_films:\n",
    "    print(f\"\\nEvaluating: {film_title_short} ({year})\")\n",
    "    \n",
    "    # Get ground truth\n",
    "    gt_row = ground_truth_df[\n",
    "        (ground_truth_df['Title'].str.contains(film_title_short, case=False)) & \n",
    "        (ground_truth_df['Year'] == year)\n",
    "    ]\n",
    "    \n",
    "    if len(gt_row) == 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Not found in ground truth\")\n",
    "        continue\n",
    "    \n",
    "    gt_row = gt_row.iloc[0]\n",
    "    \n",
    "    # Parse ground truth values\n",
    "    gt_budget = float(gt_row['Budget'].replace('$', '').replace(',', ''))\n",
    "    gt_gross = float(gt_row['Worldwide gross'].replace('$', '').replace(',', ''))\n",
    "    gt_roi = float(gt_row['ROI'])\n",
    "    \n",
    "    # Get agent's extraction\n",
    "    agent_response, citations = extract_financial_data_with_rag(film_title_short, year)\n",
    "    agent_data = parse_financial_data(agent_response)\n",
    "    \n",
    "    # Calculate errors\n",
    "    budget_error = None\n",
    "    gross_error = None\n",
    "    roi_error = None\n",
    "    \n",
    "    if agent_data['budget']:\n",
    "        budget_error = abs(agent_data['budget'] - gt_budget) / gt_budget * 100\n",
    "    \n",
    "    if agent_data['gross']:\n",
    "        gross_error = abs(agent_data['gross'] - gt_gross) / gt_gross * 100\n",
    "    \n",
    "    if agent_data['roi']:\n",
    "        roi_error = abs(agent_data['roi'] - gt_roi) / gt_roi * 100\n",
    "    \n",
    "    result = {\n",
    "        'film': film_title_short,\n",
    "        'year': year,\n",
    "        'gt_budget': gt_budget,\n",
    "        'agent_budget': agent_data['budget'],\n",
    "        'budget_error_pct': budget_error,\n",
    "        'gt_gross': gt_gross,\n",
    "        'agent_gross': agent_data['gross'],\n",
    "        'gross_error_pct': gross_error,\n",
    "        'gt_roi': gt_roi,\n",
    "        'agent_roi': agent_data['roi'],\n",
    "        'roi_error_pct': roi_error,\n",
    "        'agent_response': agent_response[:200],\n",
    "        'citations_count': len(citations) if citations else 0\n",
    "    }\n",
    "    \n",
    "    evaluation_results.append(result)\n",
    "    \n",
    "    print(f\"  Budget: ${gt_budget/1e6:.0f}M (GT) vs ${agent_data['budget']/1e6:.0f}M (Agent) - Error: {budget_error:.1f}%\" if agent_data['budget'] else \"  Budget: Not extracted\")\n",
    "    print(f\"  Gross: ${gt_gross/1e6:.0f}M (GT) vs ${agent_data['gross']/1e6:.0f}M (Agent) - Error: {gross_error:.1f}%\" if agent_data['gross'] else \"  Gross: Not extracted\")\n",
    "    print(f\"  ROI: {gt_roi:.1f}% (GT) vs {agent_data['roi']:.1f}% (Agent) - Error: {roi_error:.1f}%\" if agent_data['roi'] else \"  ROI: Not calculated\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GROUND TRUTH EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall accuracy metrics\n",
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Data extraction success rate\n",
    "budget_extracted = eval_df['agent_budget'].notna().sum()\n",
    "gross_extracted = eval_df['agent_gross'].notna().sum()\n",
    "roi_calculated = eval_df['agent_roi'].notna().sum()\n",
    "\n",
    "print(f\"\\nData Extraction Success Rate:\")\n",
    "print(f\"  Budget extracted: {budget_extracted}/{len(eval_df)} ({budget_extracted/len(eval_df)*100:.0f}%)\")\n",
    "print(f\"  Gross extracted: {gross_extracted}/{len(eval_df)} ({gross_extracted/len(eval_df)*100:.0f}%)\")\n",
    "print(f\"  ROI calculated: {roi_calculated}/{len(eval_df)} ({roi_calculated/len(eval_df)*100:.0f}%)\")\n",
    "\n",
    "# Accuracy of extracted values (within 5% is considered accurate)\n",
    "budget_accurate = (eval_df['budget_error_pct'] < 5).sum()\n",
    "gross_accurate = (eval_df['gross_error_pct'] < 5).sum()\n",
    "roi_accurate = (eval_df['roi_error_pct'] < 5).sum()\n",
    "\n",
    "print(f\"\\nExtraction Accuracy (within 5% of ground truth):\")\n",
    "print(f\"  Budget accurate: {budget_accurate}/{budget_extracted} ({budget_accurate/budget_extracted*100:.0f}%)\" if budget_extracted > 0 else \"  Budget accurate: N/A\")\n",
    "print(f\"  Gross accurate: {gross_accurate}/{gross_extracted} ({gross_accurate/gross_extracted*100:.0f}%)\" if gross_extracted > 0 else \"  Gross accurate: N/A\")\n",
    "print(f\"  ROI accurate: {roi_accurate}/{roi_calculated} ({roi_accurate/roi_calculated*100:.0f}%)\" if roi_calculated > 0 else \"  ROI accurate: N/A\")\n",
    "\n",
    "# Average errors\n",
    "print(f\"\\nAverage Errors:\")\n",
    "print(f\"  Budget: {eval_df['budget_error_pct'].mean():.2f}%\")\n",
    "print(f\"  Gross: {eval_df['gross_error_pct'].mean():.2f}%\")\n",
    "print(f\"  ROI: {eval_df['roi_error_pct'].mean():.2f}%\")\n",
    "\n",
    "print(f\"\\n‚ú® Average citations per response: {eval_df['citations_count'].mean():.1f}\")\n",
    "\n",
    "# Show the dataframe\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED RESULTS TABLE\")\n",
    "print(\"=\"*60)\n",
    "display(eval_df[['film', 'year', 'budget_error_pct', 'gross_error_pct', 'roi_error_pct', 'citations_count']])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
