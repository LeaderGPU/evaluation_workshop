{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz33G3t6gbOl"
   },
   "source": [
    "# RAG with Evaluation: Film Box Office ROI Calculator\n",
    "\n",
    "This notebook demonstrates an **agentic RAG system** that analyzes film financial performance by:\n",
    "- Extracting production budgets and box office grosses from Wikipedia\n",
    "- Calculating Return on Investment (ROI)\n",
    "- Classifying film performance (Blockbuster, Profitable, Break-even, Flop)\n",
    "- **Evaluating** against ground truth data\n",
    "\n",
    "This goes beyond simple question-answeringâ€”it requires **multi-step reasoning, calculations, and decision-making**.\n",
    "\n",
    "## Why This Use Case?\n",
    "\n",
    "Unlike simple Q&A, this agent must:\n",
    "1. **Extract** budget and gross from unstructured text\n",
    "2. **Calculate** ROI = (gross - budget) / budget Ã— 100\n",
    "3. **Classify** performance based on thresholds\n",
    "4. **Verify** calculations against ground truth\n",
    "\n",
    "At Cohere, all RAG calls come with... **precise citations**! ðŸŽ‰\n",
    "The model cites which groups of words, in the RAG chunks, were used to generate the final answer.  \n",
    "These citations make it easy to check where the model's generated response claims are coming from.\n",
    "\n",
    "**Note:** The baseline agent and eval harness are intentionally minimal to demonstrate systematic improvement through the eval-driven development cycle.\n",
    "\n",
    "RAG consists of 3 steps:\n",
    "- Step 1: Index film Wikipedia pages and retrieve relevant chunks\n",
    "- Step 2: Optionally, rerank the retrieved chunks\n",
    "- Step 3: Generate financial analysis with **precise citations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSB0pnt0gbOo"
   },
   "source": [
    "## Step 0 - Imports & Getting Film Data\n",
    "\n",
    "In this example, we'll use Wikipedia pages for multiple high-grossing films.   \n",
    "\n",
    "We'll fetch individual film pages (e.g., \"Avatar (2009 film)\", \"Avengers: Endgame\") which contain:\n",
    "- Production budgets\n",
    "- Box office performance\n",
    "- Production details\n",
    "- Release information\n",
    "\n",
    "The agent will need to extract this information and perform calculations independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rACbepFGgbOo",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:59:52.457013Z",
     "start_time": "2025-11-18T12:59:52.449162Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "# pip install cohere\n",
    "\n",
    "import cohere\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import httpx\n",
    "\n",
    "# Create an insecure HTTP client\n",
    "insecure_client = httpx.Client(verify=False)\n",
    "# Initialize Cohere client\n",
    "# Try to load from .env file first\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('../.env')\n",
    "    print('âœ… Loaded API key from .env file')\n",
    "except ImportError:\n",
    "    print('âš ï¸  python-dotenv not installed')\n",
    "\n",
    "api_key = os.environ.get('COHERE_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    api_key = input(\"Enter your Cohere API key: \")\n",
    "    os.environ['COHERE_API_KEY'] = api_key\n",
    "\n",
    "co = cohere.ClientV2(api_key=api_key,httpx_client=insecure_client)\n",
    "print('âœ… Cohere client initialized')\n",
    "# Get your free API key: https://dashboard.cohere.com/api-keys\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded API key from .env file\n",
      "âœ… Cohere client initialized\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUph1JX41665",
    "outputId": "6c63a93f-6999-47af-e704-d4a88727bc75",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:59:52.756935Z",
     "start_time": "2025-11-18T12:59:52.750989Z"
    }
   },
   "source": [
    "# For chunking let's use langchain to help us split the text\n",
    "# ! pip install -qU langchain-text-splitters -qq\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:59:53.052178Z",
     "start_time": "2025-11-18T12:59:53.042423Z"
    }
   },
   "source": [
    "with open(\"../data/rag_knowledge.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhXW7iHC1-Q6",
    "outputId": "d68ac348-4b73-4c6a-a445-6c510bdb0881",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:59:53.350022Z",
     "start_time": "2025-11-18T12:59:53.329769Z"
    }
   },
   "source": [
    "# Create basic configurations to chunk the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Split the text into chunks with some overlap\n",
    "chunks_ = text_splitter.create_documents([text])\n",
    "chunks = [c.page_content for c in chunks_]\n",
    "print(f\"The text has been broken down in {len(chunks)} chunks.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has been broken down in 1700 chunks.\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8g0sE2hgbOs"
   },
   "source": [
    "### Embed every text chunk\n",
    "\n",
    "Cohere embeddings are state-of-the-art.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEarMPEqgbOs",
    "outputId": "7da0e06d-f637-4470-8e01-6de8249be64b",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-18T13:04:01.352788Z"
    }
   },
   "source": [
    "# Because the texts being embedded are the chunks we are searching over, we set the input type as search_doc\n",
    "\n",
    "model = \"embed-v4.0\"\n",
    "\n",
    "def batch_embed(texts, batch_size=96):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        response = co.embed(\n",
    "            texts=batch,\n",
    "            model=model,\n",
    "            input_type=\"search_document\",\n",
    "            embedding_types=['float']\n",
    "        )\n",
    "        all_embeddings.extend(response.embeddings.float)\n",
    "        time.sleep(5)\n",
    "    return all_embeddings\n",
    "\n",
    "embeddings = batch_embed(chunks)\n",
    "print(f\"We just computed {len(embeddings)} embeddings.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM6vKeypgbOs"
   },
   "source": [
    "### Store the embeddings in a vector database\n",
    "\n",
    "We use the simplest vector database ever: a python dictionary using `np.array()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H2srFH-IgbOs",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:23:26.349623Z",
     "start_time": "2025-11-18T12:23:26.291157Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "vector_database = {i: np.array(embedding) for i, embedding in enumerate(embeddings)}\n",
    "# { 0: array([...]), 1: array([...]), 2: array([...]), ..., 10: array([...]) }\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "id": "sdW7M8HLvB-9",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:23:26.284655Z",
     "start_time": "2025-11-18T12:23:10.425905Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "# We use the simplest vector database ever: a python dictionary\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6NGVurZgbOs"
   },
   "source": [
    "## Given a user query, retrieve the relevant chunks from the vector database\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC05yJQ7jlek"
   },
   "source": [
    "### Define the user question\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y2HTxspKgbOs",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:45:34.041699Z",
     "start_time": "2025-11-18T12:45:34.038496Z"
    }
   },
   "source": [
    "query = \"What level is fireball\"\n",
    "query = \"Can you cast fireball at Level 0?\"\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oULg1tOjjOW"
   },
   "source": [
    "### Embed the user question\n",
    "\n",
    "Cohere embeddings are state-of-the-art.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrUuS6vXgbOs",
    "outputId": "0c64a930-f817-43c2-d775-1d9145cb304e",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:45:35.013797Z",
     "start_time": "2025-11-18T12:45:34.696076Z"
    }
   },
   "source": [
    "# Because the text being embedded is the search query, we set the input type as search_query\n",
    "response = co.embed(\n",
    "    texts=[query],\n",
    "    model=model,\n",
    "    input_type=\"search_query\",\n",
    "    embedding_types=['float']\n",
    ")\n",
    "query_embedding = response.embeddings.float[0]\n",
    "print(\"query_embedding: \", query_embedding[:10] + [\"...\"])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding:  [-0.04140373, 0.031754553, 0.022807138, -0.008070218, -0.0066667018, 0.014561481, -0.014210601, -0.027193125, -0.028070325, -0.0075000394, '...']\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K8B87CGgbOt"
   },
   "source": [
    "### Retrieve the most relevant chunks from the vector database\n",
    "\n",
    "We use cosine similarity to find the most similar chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nik3es32gbOt",
    "outputId": "a1c30024-52e1-42c7-8836-a2c590559aca",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:45:35.462122Z",
     "start_time": "2025-11-18T12:45:35.267984Z"
    }
   },
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_response_text(response) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from Cohere response.\n",
    "    Handles both reasoning models (command-a-reasoning-*) and non-reasoning models.\n",
    "    \"\"\"\n",
    "    for item in response.message.content:\n",
    "        if item.type == 'text':\n",
    "            return item.text\n",
    "    return \"\"\n",
    "\n",
    "# Calculate similarity between the user question & each chunk\n",
    "similarities = [cosine_similarity(query_embedding, chunk) for chunk in embeddings]\n",
    "print(f\"Calculated similarity for {len(similarities)} chunks (top score: {max(similarities):.4f})\")\n",
    "\n",
    "# Get indices of the top 20 most similar chunks\n",
    "sorted_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "# Keep only the top 20 indices\n",
    "top_indices = sorted_indices[:20]\n",
    "print(f\"Top 20 chunk indices: {list(top_indices[:5])} ... (showing first 5)\")\n",
    "\n",
    "# Retrieve the top 20 most similar chunks\n",
    "top_chunks_after_retrieval = [chunks[i] for i in top_indices]\n",
    "print(f\"Retrieved {len(top_chunks_after_retrieval)} chunks. Here are the top 3:\")\n",
    "for t in top_chunks_after_retrieval[:3]:\n",
    "    print(\"== \" + t)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated similarity for 941 chunks (top score: 0.4916)\n",
      "Top 20 chunk indices: [370, 380, 371, 372, 245] ... (showing first 5)\n",
      "Retrieved 20 chunks. Here are the top 3:\n",
      "== Fireball \n",
      "3rd-level evocation \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: 150 feet \n",
      "\n",
      "Components: V, S, M (a tiny ball of bat \n",
      "guano and sulfur) \n",
      "\n",
      "Duration: Instantaneous \n",
      "\n",
      "A bright streak flashes from your pointing finger to a \n",
      "point you choose within range and then blossoms with \n",
      "a low roar into an explosion of flame. Each creature \n",
      "in a 20-foot-radius sphere centered on that point must \n",
      "make a Dexterity saving throw. A target takes 8d6 fire \n",
      "\n",
      "\n",
      "\n",
      "damage on a failed save, or half as much damage on a \n",
      "successful one.\n",
      "== At Higher Levels. When you cast this spell using a \n",
      "spell slot of 6th level or higher, the fire damage or the \n",
      "radiant damage (your choice) increases by ld6 for each \n",
      "slot level above 5th. \n",
      "\n",
      "Flaming Sphere \n",
      "\n",
      "2nd-Ievel conjuration \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: 60 feet \n",
      "\n",
      "Components: V, S, M (a bit of tallow, a pinch of \n",
      "brimstone, and a dusting of powdered iron) \n",
      "\n",
      "Duration: Concentration, up to 1 minute\n",
      "== The fire spreads around corners. It ignites flammable \n",
      "objects in the area that aren't being worn or carried. \n",
      "\n",
      "At Higher Levels. When you cast this spell using a \n",
      "spell slot of 4th level or higher, the damage increases by \n",
      "ld6 for each slot level above 3rd. \n",
      "\n",
      "Fire Bolt \n",
      "Evocation cantrip \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: 120 feet \n",
      "Components: V, S \n",
      "Duration: Instantaneous\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzcpds3VgbOt"
   },
   "source": [
    "## Step 2 - Rerank the chunks retrieved from the vector database\n",
    "\n",
    "We rerank the 10 chunks retrieved from the vector database. Reranking boosts retrieval accuracy.\n",
    "\n",
    "Reranking lets us go from 10 chunks retrieved from the vector database, to the 3 most relevant chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2J4LywVygbOt",
    "outputId": "7a4c89bf-fc5e-409f-9304-fce006b9d8bf",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:45:37.025470Z",
     "start_time": "2025-11-18T12:45:36.804057Z"
    }
   },
   "source": [
    "response = co.rerank(\n",
    "    query=query,\n",
    "    documents=top_chunks_after_retrieval,\n",
    "    top_n=5,\n",
    "    model=\"rerank-v3.5\",\n",
    ")\n",
    "\n",
    "# top_chunks_after_rerank = [result.document['text'] for result in response]\n",
    "\n",
    "top_chunks_after_rerank = [top_chunks_after_retrieval[result.index] for result in response.results]\n",
    "\n",
    "print(\"Here are the top 5 chunks after rerank: \")\n",
    "for t in top_chunks_after_rerank:\n",
    "    print(\"== \" + t)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 5 chunks after rerank: \n",
      "== Fireball \n",
      "3rd-level evocation \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: 150 feet \n",
      "\n",
      "Components: V, S, M (a tiny ball of bat \n",
      "guano and sulfur) \n",
      "\n",
      "Duration: Instantaneous \n",
      "\n",
      "A bright streak flashes from your pointing finger to a \n",
      "point you choose within range and then blossoms with \n",
      "a low roar into an explosion of flame. Each creature \n",
      "in a 20-foot-radius sphere centered on that point must \n",
      "make a Dexterity saving throw. A target takes 8d6 fire \n",
      "\n",
      "\n",
      "\n",
      "damage on a failed save, or half as much damage on a \n",
      "successful one.\n",
      "== You send negative energy coursing through a creature \n",
      "that you can see within range, causing it searing pain. \n",
      "The target must make a Constitution saving throw. It \n",
      "takes 7d8 + 30 necrotic damage on a failed save, or half \n",
      "as much damage on a successful one. \n",
      "\n",
      "A humanoid killed by this spell rises at the start of \n",
      "your next turn as a zombie that is permanently under \n",
      "your command, following your verbal orders to the best \n",
      "of its ability. \n",
      "\n",
      "Fireball \n",
      "3rd-level evocation\n",
      "== You can also attack with the flame, although doing so \n",
      "ends the spell. When you cast this spell, or as an action \n",
      "on a later turn, you can hurl the flame at a creature \n",
      "within 30 feet of you. Make a ranged spell attack. On a \n",
      "hit, the target takes IdS fire damage. \n",
      "\n",
      "This spellâ€™s damage increases by IdS when you reach \n",
      "5th level (2d8), 11th level (3d8), and 17th level (4d8). \n",
      "\n",
      "Programmed Illusion \n",
      "6th-level illusion \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: 120 feet\n",
      "== If the spell is still in effect when the target is subjected \n",
      "to an effect that would kill it instantaneously without \n",
      "dealing damage, that effect is instead negated against \n",
      "the target, and the spell ends. \n",
      "\n",
      "Delayed Blast Fireball \n",
      "\n",
      "7th-Ievel evocation \n",
      "\n",
      "Casting Time: 1 action \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Range: 150 feet \n",
      "\n",
      "Components: V, S, M (a tiny ball of bat \n",
      "guano and sulfur) \n",
      "\n",
      "Duration: Concentration, up to 1 minute\n",
      "== You hurl a mote of fire at a creature or object within \n",
      "range. Make a ranged spell attack against the \n",
      "target. On a hit, the target takes IdlO fire damage. A \n",
      "flammable object hit by this spell ignites if it isn't being \n",
      "worn or carried. \n",
      "\n",
      "This spell's damage increases by Id 10 when you reach \n",
      "5th level (2dl0), 11th level (3dl0), and 17th level (4dl0). \n",
      "\n",
      "Fire Shield \n",
      "4th-IeveI evocation \n",
      "\n",
      "Casting Time: 1 action \n",
      "Range: Self \n",
      "\n",
      "Components: V. S, M (a bit of phosphorus or a firefly) \n",
      "\n",
      "Duration: 10 minutes\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuPL0VUXgbOt"
   },
   "source": [
    "## Step 3 - Generate the model final answer, given the retrieved and reranked chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oCNXWH8GgbOt",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:45:38.186185Z",
     "start_time": "2025-11-18T12:45:38.181430Z"
    }
   },
   "source": [
    "# preamble containing instructions about the task and the desired style for the output.\n",
    "preamble = \"\"\"\n",
    "You are the D&D Loremaster, an expert Dungeon Master assistant designed to provide authoritative, accurate, and actionable answers about Dungeons & Dragons.\n",
    "\n",
    "Your primary purpose is to help users understand rules, mechanics, lore, character creation, items, spells, monsters, worldbuilding, and gameplay.\n",
    "You have access to a Retrieval-Augmented Generation (RAG) resource, referred to as the Handbook, which may contain rules text, reference material, or worldbuilding documents.\n",
    "\n",
    "When responding, always integrate relevant information retrieved from the Handbook when available.\n",
    "\n",
    "If the RAG system returns relevant passages, you MUST incorporate them correctly, consistently, and in a way that answers the userâ€™s question completely\n",
    "\n",
    "## Task & Context\n",
    "You help people answer their questions about rules of the roleplaying game Dungeons and Dragons. You will be asked a very wide array of requests on all kinds of topics relating to the rules of Dungeons and Dragons. You will have access to a search function over the entire rulesbook. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n",
    "The question does not always contain all the information that is needed to answer the question, in that case ask the user for more information.\n",
    "\n",
    "## Style Guide\n",
    "Please return the minimun amount of information, if the user asks for a number answer only with the number. Otherwise keep the answer short.\n",
    "\"\"\"\n",
    "\n",
    "new_preamble = \"\"\"\n",
    "You are an expert in answering Dungeons and Dragons rules quesitons. Answer te questions in the following JSON format:\n",
    "{\n",
    "\n",
    "}\n",
    "\"\"\"\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BevatShtgbOt",
    "outputId": "af71f4a9-787a-4ee3-9598-20692fb3bf16",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:45:40.654931Z",
     "start_time": "2025-11-18T12:45:38.742135Z"
    }
   },
   "source": [
    "# retrieved documents - now using all 5 reranked chunks\n",
    "documents = [\n",
    "    {\"data\": {\"title\": f\"chunk {i}\", \"snippet\": chunk}} \n",
    "    for i, chunk in enumerate(top_chunks_after_rerank)\n",
    "]\n",
    "\n",
    "print(f\"Passing {len(documents)} documents to the model\")\n",
    "\n",
    "# get model response\n",
    "response = co.chat(\n",
    "  model=\"command-a-reasoning-08-2025\",\n",
    "  messages=[{\"role\" : \"system\", \"content\" : preamble},\n",
    "            {\"role\" : \"user\", \"content\" : query}],\n",
    "  documents=documents,  \n",
    "  temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"Final answer:\")\n",
    "print(get_response_text(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing 5 documents to the model\n",
      "Final answer:\n",
      "No, Fireball is a 3rd level evocation therefore you must be at least level 3 to cast it.\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating the judge"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:49:14.531662Z",
     "start_time": "2025-11-18T12:49:14.520291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"../data/fullrules5e.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(len(text))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198079\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhXW7iHC1-Q6",
    "outputId": "d68ac348-4b73-4c6a-a445-6c510bdb0881",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:51:03.059851Z",
     "start_time": "2025-11-18T12:51:02.903579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create basic configurations to chunk the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Split the text into chunks with some overlap\n",
    "chunks_ = text_splitter.create_documents([text])\n",
    "chunks = [c.page_content for c in chunks_]\n",
    "print(f\"The text has been broken down in {len(chunks)} chunks.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has been broken down in 3143 chunks.\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Embed every text chunk\n",
    "\n",
    "Cohere embeddings are state-of-the-art.\n"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEarMPEqgbOs",
    "outputId": "7da0e06d-f637-4470-8e01-6de8249be64b",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:52:22.799872Z",
     "start_time": "2025-11-18T12:52:11.417537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Because the texts being embedded are the chunks we are searching over, we set the input type as search_doc\n",
    "\n",
    "model = \"embed-v4.0\"\n",
    "\n",
    "full_embeddings = batch_embed(chunks)\n",
    "print(f\"We just computed {len(embeddings)} embeddings.\")\n"
   ],
   "outputs": [
    {
     "ename": "TooManyRequestsError",
     "evalue": "headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-encoding': 'gzip', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin,Accept-Encoding', 'x-accel-expires': '0', 'x-debug-trace-id': 'e6977cbaf022a4639178ca660fa2775b', 'x-endpoint-monthly-call-limit': '1000', 'x-trial-endpoint-call-limit': '40', 'x-trial-endpoint-call-remaining': '29', 'date': 'Tue, 18 Nov 2025 12:52:22 GMT', 'x-envoy-upstream-service-time': '21', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, status_code: 429, body: {'id': '91dfdccd-5fa5-41bd-bf9f-9a9798dcb144', 'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTooManyRequestsError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Because the texts being embedded are the chunks we are searching over, we set the input type as search_doc\u001B[39;00m\n\u001B[0;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membed-v4.0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 5\u001B[0m full_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mbatch_embed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe just computed \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(embeddings)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m embeddings.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[6], line 9\u001B[0m, in \u001B[0;36mbatch_embed\u001B[1;34m(texts, batch_size)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(texts), batch_size):\n\u001B[0;32m      8\u001B[0m     batch \u001B[38;5;241m=\u001B[39m texts[i:i\u001B[38;5;241m+\u001B[39mbatch_size]\n\u001B[1;32m----> 9\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mco\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msearch_document\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43membedding_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfloat\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     all_embeddings\u001B[38;5;241m.\u001B[39mextend(response\u001B[38;5;241m.\u001B[39membeddings\u001B[38;5;241m.\u001B[39mfloat)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m all_embeddings\n",
      "File \u001B[1;32m~\\Applications\\python-3.12\\Lib\\site-packages\\cohere\\v2\\client.py:480\u001B[0m, in \u001B[0;36mV2Client.embed\u001B[1;34m(self, model, input_type, texts, images, inputs, max_tokens, output_dimension, embedding_types, truncate, priority, request_options)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21membed\u001B[39m(\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    389\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    400\u001B[0m     request_options: typing\u001B[38;5;241m.\u001B[39mOptional[RequestOptions] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    401\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m EmbedByTypeResponse:\n\u001B[0;32m    402\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    403\u001B[0m \u001B[38;5;124;03m    This endpoint returns text embeddings. An embedding is a list of floating point numbers that captures semantic information about the text that it represents.\u001B[39;00m\n\u001B[0;32m    404\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;124;03m    )\u001B[39;00m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 480\u001B[0m     _response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    486\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_dimension\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_dimension\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[43membedding_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpriority\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpriority\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _response\u001B[38;5;241m.\u001B[39mdata\n",
      "File \u001B[1;32m~\\Applications\\python-3.12\\Lib\\site-packages\\cohere\\v2\\raw_client.py:877\u001B[0m, in \u001B[0;36mRawV2Client.embed\u001B[1;34m(self, model, input_type, texts, images, inputs, max_tokens, output_dimension, embedding_types, truncate, priority, request_options)\u001B[0m\n\u001B[0;32m    866\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m UnprocessableEntityError(\n\u001B[0;32m    867\u001B[0m         headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(_response\u001B[38;5;241m.\u001B[39mheaders),\n\u001B[0;32m    868\u001B[0m         body\u001B[38;5;241m=\u001B[39mtyping\u001B[38;5;241m.\u001B[39mcast(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    874\u001B[0m         ),\n\u001B[0;32m    875\u001B[0m     )\n\u001B[0;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m429\u001B[39m:\n\u001B[1;32m--> 877\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m TooManyRequestsError(\n\u001B[0;32m    878\u001B[0m         headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(_response\u001B[38;5;241m.\u001B[39mheaders),\n\u001B[0;32m    879\u001B[0m         body\u001B[38;5;241m=\u001B[39mtyping\u001B[38;5;241m.\u001B[39mcast(\n\u001B[0;32m    880\u001B[0m             typing\u001B[38;5;241m.\u001B[39mOptional[typing\u001B[38;5;241m.\u001B[39mAny],\n\u001B[0;32m    881\u001B[0m             construct_type(\n\u001B[0;32m    882\u001B[0m                 type_\u001B[38;5;241m=\u001B[39mtyping\u001B[38;5;241m.\u001B[39mOptional[typing\u001B[38;5;241m.\u001B[39mAny],  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m    883\u001B[0m                 object_\u001B[38;5;241m=\u001B[39m_response\u001B[38;5;241m.\u001B[39mjson(),\n\u001B[0;32m    884\u001B[0m             ),\n\u001B[0;32m    885\u001B[0m         ),\n\u001B[0;32m    886\u001B[0m     )\n\u001B[0;32m    887\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m498\u001B[39m:\n\u001B[0;32m    888\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidTokenError(\n\u001B[0;32m    889\u001B[0m         headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(_response\u001B[38;5;241m.\u001B[39mheaders),\n\u001B[0;32m    890\u001B[0m         body\u001B[38;5;241m=\u001B[39mtyping\u001B[38;5;241m.\u001B[39mcast(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    896\u001B[0m         ),\n\u001B[0;32m    897\u001B[0m     )\n",
      "\u001B[1;31mTooManyRequestsError\u001B[0m: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-encoding': 'gzip', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin,Accept-Encoding', 'x-accel-expires': '0', 'x-debug-trace-id': 'e6977cbaf022a4639178ca660fa2775b', 'x-endpoint-monthly-call-limit': '1000', 'x-trial-endpoint-call-limit': '40', 'x-trial-endpoint-call-remaining': '29', 'date': 'Tue, 18 Nov 2025 12:52:22 GMT', 'x-envoy-upstream-service-time': '21', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, status_code: 429, body: {'id': '91dfdccd-5fa5-41bd-bf9f-9a9798dcb144', 'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Store the embeddings in a vector database\n",
    "\n",
    "We use the simplest vector database ever: a python dictionary using `np.array()`.\n"
   ]
  },
  {
   "metadata": {
    "id": "H2srFH-IgbOs",
    "ExecuteTime": {
     "end_time": "2025-11-18T12:23:26.349623Z",
     "start_time": "2025-11-18T12:23:26.291157Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 8,
   "source": [
    "import numpy as np\n",
    "vector_database = {i: np.array(embedding) for i, embedding in enumerate(embeddings)}\n",
    "# { 0: array([...]), 1: array([...]), 2: array([...]), ..., 10: array([...]) }\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:45:40.671470Z",
     "start_time": "2025-11-18T12:45:40.666895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "judge_prompt = \"\"\"\n",
    "You are an AI assistant responsible for checking the answers of another LLM DnD chatbot. You will receive additional context from the official rules. Answer only with \"True\" if you consider the answer correct or \"False\" if you have any doubt about the answer. Do not add any other formatting to your answer\n",
    "\"\"\"\n",
    "\n",
    "judge_query = f\"\"\"\n",
    "This is what the user asked: {query}\n",
    "\n",
    "This is the llm response: {get_response_text(response)}\n",
    "\n",
    "Use the available context information\n",
    "\"\"\"\n"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:45:42.607326Z",
     "start_time": "2025-11-18T12:45:40.713002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response_judge = co.chat(\n",
    "  model=\"command-a-reasoning-08-2025\",\n",
    "  messages=[{\"role\" : \"system\", \"content\" : judge_prompt},\n",
    "            {\"role\" : \"user\", \"content\" : judge_query}],\n",
    "  documents=documents,\n",
    "  temperature=0.3\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:45:42.624992Z",
     "start_time": "2025-11-18T12:45:42.618767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Judge answer:\")\n",
    "print(get_response_text(response_judge))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge answer:\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20wcn-EjlXZd"
   },
   "source": [
    "Note: this is indeed the answer you'd expect, and here was the passage of text in wikipedia explaining it!\n",
    "\n",
    "\" [...] Star Wars was originally scheduled to be released on October 20, 2023, but was delayed to November 17, 2023, before moving forward two weeks to November 3, 2023, to adjust to changes in release schedules from other studios. It was later postponed by over four months to March 15, 2024, due to the 2023 Hollywood labor disputes. After the strikes were resolved, the film moved once more up two weeks to March 1, 2024. [...]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoSVDXSsgbOt"
   },
   "source": [
    "## Bonus: Citations come for free with Cohere! ðŸŽ‰\n",
    "\n",
    "At Cohere, all RAG calls come with... precise citations! ðŸŽ‰\n",
    "The model cites which groups of words, in the RAG chunks, were used to generate the final answer.  \n",
    "These citations make it easy to check where the modelâ€™s generated response claims are coming from.  \n",
    "They help users gain visibility into the model reasoning, and sanity check the final model generation.  \n",
    "These citations are optional â€” you can decide to ignore them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVTuQdmDgbOt",
    "outputId": "f843b262-d8bb-45ba-cbfb-9915da104eda"
   },
   "outputs": [],
   "source": [
    "print(\"Citations that support the final answer:\")\n",
    "for cite in response.message.citations:\n",
    "    print(cite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IueXaIJggbOu",
    "outputId": "c816af51-74be-42c9-e94e-9820bbf95f79"
   },
   "outputs": [],
   "source": [
    "def insert_inline_citations(text, citations, field='text'):\n",
    "    sorted_citations = sorted(citations, key=lambda c: c.start, reverse=True)\n",
    "    \n",
    "    for citation in sorted_citations:\n",
    "        source_ids = [source.id.split(':')[-1] for source in citation.sources]\n",
    "        citation_text = f\"[{','.join(source_ids)}]\"\n",
    "        text = text[:citation.end] + citation_text + text[citation.end:]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def list_sources(citations, fields=['text']):\n",
    "    unique_sources = set()\n",
    "    for citation in citations:\n",
    "        for source in citation.sources:\n",
    "            source_data = tuple((field, source.document[field]) for field in fields if field in source.document)\n",
    "            unique_sources.add((source.id.split(':')[-1], source_data))\n",
    "    \n",
    "    footnotes = []\n",
    "    for source_id, source_data in sorted(unique_sources):\n",
    "        footnote = f\"[{source_id}] \" + \", \".join(f\"{key}: {value}\" for key, value in source_data)\n",
    "        footnotes.append(footnote)\n",
    "    \n",
    "    return \"\\n\".join(footnotes)\n",
    "\n",
    "# Use the functions\n",
    "cited_text = insert_inline_citations(response.message.content[1].text, response.message.citations)\n",
    "\n",
    "# Print the result with inline citations\n",
    "print(cited_text)\n",
    "\n",
    "# Print footnotes\n",
    "if response.message.citations:\n",
    "    print(\"\\nSource documents:\")\n",
    "    print(list_sources(response.message.citations, fields=['title','snippet']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kp4c_HkYIEn_"
   },
   "outputs": [],
   "source": [
    "def insert_inline_citations(text, citations, field='text'):\n",
    "    sorted_citations = sorted(citations, key=lambda c: c.start, reverse=True)\n",
    "    \n",
    "    for citation in sorted_citations:\n",
    "        source_ids = [source.id.split(':')[-1] for source in citation.sources]\n",
    "        citation_text = f\"[{','.join(source_ids)}]\"\n",
    "        text = text[:citation.end] + citation_text + text[citation.end:]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def list_sources(citations):\n",
    "    unique_sources = set()\n",
    "    for citation in citations:\n",
    "        for source in citation.sources:\n",
    "            source_data = tuple((key, value) for key, value in source.document.items() if key != 'id')\n",
    "            unique_sources.add((source.id.split(':')[-1], source_data))\n",
    "    \n",
    "    footnotes = []\n",
    "    for source_id, source_data in sorted(unique_sources):\n",
    "        footnote = f\"[{source_id}] \" + \", \".join(f\"{key}: {value}\" for key, value in source_data)\n",
    "        footnotes.append(footnote)\n",
    "    \n",
    "    return \"\\n\".join(footnotes)\n",
    "\n",
    "# Use the functions\n",
    "cited_text = insert_inline_citations(response.message.content[1].text, response.message.citations)\n",
    "\n",
    "# Print the result with inline citations\n",
    "print(cited_text)\n",
    "\n",
    "# Print footnotes\n",
    "if response.message.citations:\n",
    "    print(\"\\nSource documents:\")\n",
    "    print(list_sources(response.message.citations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: RAG Evaluation\n",
    "\n",
    "Now that we've built a RAG system, let's evaluate its performance!\n",
    "\n",
    "We'll create test queries about Star Wars and measure how well our RAG system answers them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluation Section\n",
    "\n",
    "Now we'll evaluate our agent's ability to extract financial data and calculate ROI correctly.\n",
    "\n",
    "## Define Test Queries\n",
    "\n",
    "We'll create test queries that ask the agent to analyze films and extract their financial performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for Film ROI Analysis\n",
    "# These match the films fetched from Wikipedia and include ground truth values from CSV\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What was Avengers: Endgame's budget and how much did it gross worldwide? What was the ROI?\",\n",
    "        \"film_title\": \"Avengers: Endgame\",\n",
    "        \"year\": 2019,\n",
    "        \"budget\": 356.0,              # Ground truth from CSV (millions)\n",
    "        \"worldwide_gross\": 2797.5,    # Ground truth from CSV (millions)\n",
    "        \"roi\": 685.81,                # Ground truth from CSV (percent)\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How much did Titanic cost to make and how much money did it earn at the box office?\",\n",
    "        \"film_title\": \"Titanic\",\n",
    "        \"year\": 1997,\n",
    "        \"budget\": 200.0,              # Ground truth from CSV (millions)\n",
    "        \"worldwide_gross\": 2257.9,    # Ground truth from CSV (millions)\n",
    "        \"roi\": 1028.95,               # Ground truth from CSV (percent)\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What was the production budget and worldwide box office gross for Star Wars: The Force Awakens?\",\n",
    "        \"film_title\": \"Star Wars: The Force Awakens\",\n",
    "        \"year\": 2015,\n",
    "        \"budget\": 245.0,              # Ground truth from CSV (millions)\n",
    "        \"worldwide_gross\": 2068.2,    # Ground truth from CSV (millions)\n",
    "        \"roi\": 744.17,                # Ground truth from CSV (percent)\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What was the budget and worldwide gross for Avengers: Infinity War?\",\n",
    "        \"film_title\": \"Avengers: Infinity War\",\n",
    "        \"year\": 2018,\n",
    "        \"budget\": 400.0,              # Ground truth from CSV (millions)\n",
    "        \"worldwide_gross\": 2048.4,    # Ground truth from CSV (millions)\n",
    "        \"roi\": 412.09,                # Ground truth from CSV (percent)\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Tell me about Spider-Man: No Way Home's financial performance - budget, gross, and profitability.\",\n",
    "        \"film_title\": \"Spider-Man: No Way Home\",\n",
    "        \"year\": 2021,\n",
    "        \"budget\": 200.0,              # Ground truth from CSV (millions)\n",
    "        \"worldwide_gross\": 1922.6,    # Ground truth from CSV (millions)\n",
    "        \"roi\": 861.30,                # Ground truth from CSV (percent)\n",
    "        \"difficulty\": \"hard\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function\n",
    "\n",
    "This function will:\n",
    "1. Run the RAG pipeline on each query\n",
    "2. Check if expected keywords appear in the answer\n",
    "3. Evaluate citation quality\n",
    "4. Measure response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "import re\n",
    "\n",
    "def parse_financial_numbers(text: str) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Extract budget, gross, and ROI from text response\n",
    "    Returns values in millions of dollars\n",
    "    \"\"\"\n",
    "    result = {\"budget\": None, \"gross\": None, \"roi\": None}\n",
    "    \n",
    "    # Pattern for budget (in millions or billions)\n",
    "    budget_patterns = [\n",
    "        r'budget.*?\\$(\\d+(?:\\.\\d+)?)\\s*billion',\n",
    "        r'budget.*?\\$(\\d+(?:,\\d+)?(?:\\.\\d+)?)\\s*million',\n",
    "        r'\\$(\\d+(?:,\\d+)?(?:\\.\\d+)?)\\s*million.*?budget'\n",
    "    ]\n",
    "    \n",
    "    # Pattern for gross (in millions or billions)\n",
    "    gross_patterns = [\n",
    "        r'(?:gross|earned|made).*?\\$(\\d+(?:\\.\\d+)?)\\s*billion',\n",
    "        r'(?:gross|earned|made).*?\\$(\\d+(?:,\\d+)?(?:\\.\\d+)?)\\s*million',\n",
    "        r'\\$(\\d+(?:\\.\\d+)?)\\s*billion.*?(?:gross|worldwide)'\n",
    "    ]\n",
    "    \n",
    "    # Pattern for ROI percentage\n",
    "    roi_patterns = [\n",
    "        r'ROI.*?(\\d+(?:\\.\\d+)?)\\s*%',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*%.*?ROI',\n",
    "        r'return.*?(\\d+(?:\\.\\d+)?)\\s*%'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Extract budget\n",
    "    for pattern in budget_patterns:\n",
    "        match = re.search(pattern, text_lower, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = float(match.group(1).replace(',', ''))\n",
    "            result[\"budget\"] = value * 1000 if 'billion' in match.group(0).lower() else value\n",
    "            break\n",
    "    \n",
    "    # Extract gross\n",
    "    for pattern in gross_patterns:\n",
    "        match = re.search(pattern, text_lower, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = float(match.group(1).replace(',', ''))\n",
    "            result[\"gross\"] = value * 1000 if 'billion' in match.group(0).lower() else value\n",
    "            break\n",
    "    \n",
    "    # Extract ROI\n",
    "    for pattern in roi_patterns:\n",
    "        match = re.search(pattern, text_lower, re.IGNORECASE)\n",
    "        if match:\n",
    "            result[\"roi\"] = float(match.group(1))\n",
    "            break\n",
    "    \n",
    "    return result\n",
    "\n",
    "def evaluate_response(response: str, expected_budget: float, expected_gross: float, expected_roi: float) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate response by comparing extracted numbers to ground truth\n",
    "    All values should be in millions\n",
    "    \"\"\"\n",
    "    parsed = parse_financial_numbers(response)\n",
    "    \n",
    "    # Calculate errors (allow 5% tolerance)\n",
    "    tolerance = 0.05\n",
    "    \n",
    "    budget_match = False\n",
    "    gross_match = False\n",
    "    roi_match = False\n",
    "    \n",
    "    budget_error = None\n",
    "    gross_error = None\n",
    "    roi_error = None\n",
    "    \n",
    "    if parsed[\"budget\"]:\n",
    "        budget_error = abs(parsed[\"budget\"] - expected_budget) / expected_budget\n",
    "        budget_match = budget_error <= tolerance\n",
    "    \n",
    "    if parsed[\"gross\"]:\n",
    "        gross_error = abs(parsed[\"gross\"] - expected_gross) / expected_gross\n",
    "        gross_match = gross_error <= tolerance\n",
    "    \n",
    "    if parsed[\"roi\"]:\n",
    "        roi_error = abs(parsed[\"roi\"] - expected_roi) / expected_roi\n",
    "        roi_match = roi_error <= tolerance\n",
    "    \n",
    "    # Pass if at least budget and gross are correct (ROI can be calculated from these)\n",
    "    passed = budget_match and gross_match\n",
    "    \n",
    "    return {\n",
    "        \"passed\": passed,\n",
    "        \"budget_match\": budget_match,\n",
    "        \"gross_match\": gross_match,\n",
    "        \"roi_match\": roi_match,\n",
    "        \"parsed_budget\": parsed[\"budget\"],\n",
    "        \"parsed_gross\": parsed[\"gross\"],\n",
    "        \"parsed_roi\": parsed[\"roi\"],\n",
    "        \"budget_error\": budget_error,\n",
    "        \"gross_error\": gross_error,\n",
    "        \"roi_error\": roi_error\n",
    "    }\n",
    "\n",
    "print(\"âœ… Evaluation function created with ground truth comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Let's test the agent on all our film financial queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, test in enumerate(test_cases):\n",
    "    print(f\"\\nEvaluating query {i+1}/{len(test_cases)}: {test['query'][:60]}...\")\n",
    "    \n",
    "    # Embed and retrieve\n",
    "    query_resp = co.embed(\n",
    "        texts=[test['query']],\n",
    "        model=model,\n",
    "        input_type=\"search_query\",\n",
    "        embedding_types=['float']\n",
    "    )\n",
    "    q_embedding = query_resp.embeddings.float[0]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sims = [cosine_similarity(q_embedding, chunk_emb) for chunk_emb in embeddings]\n",
    "    top_idx = np.argsort(sims)[-20:][::-1]\n",
    "    top_chunks = [chunks[i] for i in top_idx]\n",
    "    \n",
    "    # Rerank\n",
    "    rerank_resp = co.rerank(\n",
    "        query=test['query'],\n",
    "        documents=top_chunks,\n",
    "        top_n=5,\n",
    "        model=\"rerank-v3.5\",\n",
    "    )\n",
    "    reranked = [top_chunks[r.index] for r in rerank_resp.results]\n",
    "    \n",
    "    # Generate response\n",
    "    docs = [{\"data\": {\"snippet\": doc}} for doc in reranked]\n",
    "    chat_resp = co.chat(\n",
    "        model=\"command-a-reasoning-08-2025\",\n",
    "        messages=[{\"role\": \"user\", \"content\": test['query']}],\n",
    "        documents=docs,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    answer = get_response_text(chat_resp)\n",
    "    \n",
    "    # Evaluate against ground truth from CSV\n",
    "    eval_result = evaluate_response(\n",
    "        answer, \n",
    "        test['budget'], \n",
    "        test['worldwide_gross'], \n",
    "        test['roi']\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"query\": test['query'],\n",
    "        \"answer\": answer,\n",
    "        \"film_title\": test['film_title'],\n",
    "        \"difficulty\": test['difficulty'],\n",
    "        **eval_result\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    status = \"âœ… PASS\" if eval_result['passed'] else \"âŒ FAIL\"\n",
    "    budget_status = \"âœ“\" if eval_result['budget_match'] else \"âœ—\"\n",
    "    gross_status = \"âœ“\" if eval_result['gross_match'] else \"âœ—\"\n",
    "    roi_status = \"âœ“\" if eval_result['roi_match'] else \"âœ—\"\n",
    "    print(f\"  {status} | Budget: {budget_status} | Gross: {gross_status} | ROI: {roi_status}\")\n",
    "\n",
    "print(f\"\\nâœ… Evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Let's see how well our agent extracted and calculated film financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "total_tests = len(results)\n",
    "passed = sum(1 for r in results if r['passed'])\n",
    "budget_correct = sum(1 for r in results if r['budget_match'])\n",
    "gross_correct = sum(1 for r in results if r['gross_match'])\n",
    "roi_correct = sum(1 for r in results if r['roi_match'])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RAG EVALUATION RESULTS - Film Box Office ROI Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total queries tested: {total_tests}\")\n",
    "print(f\"Overall pass rate: {passed}/{total_tests} ({passed/total_tests*100:.1f}%)\")\n",
    "print()\n",
    "print(f\"Accuracy by metric:\")\n",
    "print(f\"  Budget extraction:  {budget_correct}/{total_tests} ({budget_correct/total_tests*100:.1f}%)\")\n",
    "print(f\"  Gross extraction:   {gross_correct}/{total_tests} ({gross_correct/total_tests*100:.1f}%)\")\n",
    "print(f\"  ROI calculation:    {roi_correct}/{total_tests} ({roi_correct/total_tests*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Show details for failures\n",
    "failures = [r for r in results if not r['passed']]\n",
    "if failures:\n",
    "    print(f\"Failed queries: {len(failures)}\")\n",
    "    for fail in failures:\n",
    "        print(f\"  âŒ {fail['film_title']}\")\n",
    "        if fail['parsed_budget']:\n",
    "            print(f\"     Budget: ${fail['parsed_budget']:.1f}M (error: {fail['budget_error']*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"     Budget: Not found\")\n",
    "        if fail['parsed_gross']:\n",
    "            print(f\"     Gross: ${fail['parsed_gross']:.1f}M (error: {fail['gross_error']*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"     Gross: Not found\")\n",
    "else:\n",
    "    print(\"ðŸŽ‰ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Detailed Results\n",
    "\n",
    "Let's examine each query to see how the agent performed on film financial extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test {i}: {result['film_title']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"\\nPassed: {'âœ“' if result['passed'] else 'âœ—'}\")\n",
    "    # print(f\"Response time: {result['response_time']:.2f}s\")\n",
    "    # print(f\"Citations: {len(result['citations'])}\")\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{result['answer'][:500]}...\")  # First 500 chars\n",
    "    \n",
    "    print(f\"\\nKeyword check:\")\n",
    "    # for keyword in result['expected_keywords']:\n",
    "    #     found = keyword.lower() in result['answer'].lower()\n",
    "    #     print(f\"  {'âœ“' if found else 'âœ—'} '{keyword}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Summary\n",
    "\n",
    "### What We Built:\n",
    "\n",
    "**Part 1: RAG Tutorial**\n",
    "- âœ… Document chunking and embedding with `embed-v4.0`\n",
    "- âœ… Vector similarity search (cosine similarity)\n",
    "- âœ… Reranking with `rerank-v3.5`\n",
    "- âœ… Generation with `command-a-reasoning-08-2025` and **citations**\n",
    "- âœ… Citation formatting utilities\n",
    "\n",
    "**Part 2: Evaluation**\n",
    "- âœ… Test queries on the same dataset\n",
    "- âœ… Automated evaluation with keyword coverage\n",
    "- âœ… Pass/fail analysis by difficulty\n",
    "- âœ… Detailed results inspection\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **RAG improves accuracy** by grounding responses in retrieved documents\n",
    "2. **Citations build trust** by showing which sources were used\n",
    "3. **Reranking boosts performance** by prioritizing the most relevant chunks\n",
    "4. **Evaluation is essential** to measure and improve RAG systems\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Try different parameters**: Experiment with chunk sizes, top-k values, temperature\n",
    "- **Add more test queries**: Create a larger evaluation set\n",
    "- **Improve retrieval**: Try different embedding models or hybrid search\n",
    "- **Enhance evaluation**: Add more sophisticated metrics (BLEU, ROUGE, LLM-as-judge)\n",
    "\n",
    "**Happy building! ðŸš€**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Advanced Evaluation: Ground Truth Validation\n",
    "\n",
    "Now let's evaluate the agent's ability to **extract accurate financial data** and **calculate ROI correctly** by comparing against our ground truth dataset.\n",
    "\n",
    "This evaluation will measure:\n",
    "1. **Data Extraction Accuracy**: Did the agent find the correct budget and gross figures?\n",
    "2. **Calculation Correctness**: Is the ROI calculation mathematically accurate?\n",
    "3. **Classification Accuracy**: Does the performance classification match the actual ROI?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv('../data/ground_truth/film_box_office_ground_truth.csv')\n",
    "\n",
    "print(f\"Loaded {len(ground_truth_df)} films from ground truth\")\n",
    "print(\"\\nSample data:\")\n",
    "print(ground_truth_df[['Year', 'Title', 'Budget', 'Worldwide gross', 'ROI']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_financial_data_with_rag(film_title, film_year=None):\n",
    "    \"\"\"\n",
    "    Use RAG to extract budget and gross for a specific film\n",
    "    \"\"\"\n",
    "    # Construct query\n",
    "    query = f\"What was the production budget and worldwide box office gross for {film_title}?\"\n",
    "    if film_year:\n",
    "        query += f\" (from {film_year})\"\n",
    "    \n",
    "    # Embed query\n",
    "    query_embed = co.embed(\n",
    "        texts=[query],\n",
    "        model=model,\n",
    "        input_type=\"search_query\",\n",
    "        embedding_types=[\"float\"]\n",
    "    ).embeddings.float[0]\n",
    "    \n",
    "    # Retrieve chunks\n",
    "    doc_ids = [id for id, _ in sorted(\n",
    "        [(id, cosine_similarity(query_embed, emb)) for id, emb in embeddings.items()],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:20]]\n",
    "    docs_to_use = [chunks[id] for id in doc_ids]\n",
    "    \n",
    "    # Rerank\n",
    "    rerank_results = co.rerank(\n",
    "        query=query,\n",
    "        documents=docs_to_use,\n",
    "        top_n=5,\n",
    "        model=\"rerank-v3.5\"\n",
    "    )\n",
    "    \n",
    "    docs_to_use = [docs_to_use[r.index] for r in rerank_results.results]\n",
    "    \n",
    "    # Generate response with specific instructions\n",
    "    preamble = \"\"\"You are a financial analyst extracting film production data. \n",
    "    Extract the EXACT budget and worldwide box office gross figures from the provided text.\n",
    "    Report numbers in millions (e.g., '$237 million' or '$2.9 billion').\n",
    "    If you find the data, also calculate the ROI using: ROI = (gross - budget) / budget Ã— 100\"\"\"\n",
    "    \n",
    "    response = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        documents=[{\"text\": doc} for doc in docs_to_use],\n",
    "        preamble=preamble\n",
    "    )\n",
    "    \n",
    "    return get_response_text(response), response.message.citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_financial_data(agent_response):\n",
    "    \"\"\"\n",
    "    Parse budget and gross from agent's text response\n",
    "    Returns values in dollars (not millions)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Look for budget\n",
    "    budget = None\n",
    "    budget_patterns = [\n",
    "        r'budget.*?\\$([\\d,]+)\\s*million',\n",
    "        r'\\$([\\d,]+)\\s*million.*?budget',\n",
    "        r'cost.*?\\$([\\d,]+)\\s*million',\n",
    "    ]\n",
    "    \n",
    "    for pattern in budget_patterns:\n",
    "        match = re.search(pattern, agent_response, re.IGNORECASE)\n",
    "        if match:\n",
    "            budget = float(match.group(1).replace(',', '')) * 1_000_000\n",
    "            break\n",
    "    \n",
    "    # Look for gross\n",
    "    gross = None\n",
    "    gross_patterns = [\n",
    "        r'gross.*?\\$([\\d.]+)\\s*billion',\n",
    "        r'\\$([\\d.]+)\\s*billion.*?gross',\n",
    "        r'earned.*?\\$([\\d.]+)\\s*billion',\n",
    "        r'box office.*?\\$([\\d.]+)\\s*billion',\n",
    "    ]\n",
    "    \n",
    "    for pattern in gross_patterns:\n",
    "        match = re.search(pattern, agent_response, re.IGNORECASE)\n",
    "        if match:\n",
    "            gross = float(match.group(1)) * 1_000_000_000\n",
    "            break\n",
    "    \n",
    "    # If not found in billions, try millions\n",
    "    if gross is None:\n",
    "        gross_patterns_mil = [\n",
    "            r'gross.*?\\$([\\d,]+)\\s*million',\n",
    "            r'\\$([\\d,]+)\\s*million.*?gross',\n",
    "        ]\n",
    "        for pattern in gross_patterns_mil:\n",
    "            match = re.search(pattern, agent_response, re.IGNORECASE)\n",
    "            if match:\n",
    "                gross = float(match.group(1).replace(',', '')) * 1_000_000\n",
    "                break\n",
    "    \n",
    "    # Calculate ROI if we have both\n",
    "    roi = None\n",
    "    if budget and gross and budget > 0:\n",
    "        roi = ((gross - budget) / budget) * 100\n",
    "    \n",
    "    return {\n",
    "        'budget': budget,\n",
    "        'gross': gross,\n",
    "        'roi': roi\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a subset of films from ground truth (matching Wikipedia dataset)\n",
    "test_films = [\n",
    "    ('Avengers: Endgame', 2019),\n",
    "    ('Titanic', 1997),\n",
    "    ('Star Wars: The Force Awakens', 2015),\n",
    "    ('Avengers: Infinity War', 2018),\n",
    "    ('Spider-Man: No Way Home', 2021),\n",
    "]\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for film_title_short, year in test_films:\n",
    "    print(f\"\\nEvaluating: {film_title_short} ({year})\")\n",
    "    \n",
    "    # Get ground truth\n",
    "    gt_row = ground_truth_df[\n",
    "        (ground_truth_df['Title'].str.contains(film_title_short, case=False)) & \n",
    "        (ground_truth_df['Year'] == year)\n",
    "    ]\n",
    "    \n",
    "    if len(gt_row) == 0:\n",
    "        print(f\"  âš ï¸  Not found in ground truth\")\n",
    "        continue\n",
    "    \n",
    "    gt_row = gt_row.iloc[0]\n",
    "    \n",
    "    # Parse ground truth values\n",
    "    gt_budget = float(gt_row['Budget'].replace('$', '').replace(',', ''))\n",
    "    gt_gross = float(gt_row['Worldwide gross'].replace('$', '').replace(',', ''))\n",
    "    gt_roi = float(gt_row['ROI'])\n",
    "    \n",
    "    # Get agent's extraction\n",
    "    agent_response, citations = extract_financial_data_with_rag(film_title_short, year)\n",
    "    agent_data = parse_financial_data(agent_response)\n",
    "    \n",
    "    # Calculate errors\n",
    "    budget_error = None\n",
    "    gross_error = None\n",
    "    roi_error = None\n",
    "    \n",
    "    if agent_data['budget']:\n",
    "        budget_error = abs(agent_data['budget'] - gt_budget) / gt_budget * 100\n",
    "    \n",
    "    if agent_data['gross']:\n",
    "        gross_error = abs(agent_data['gross'] - gt_gross) / gt_gross * 100\n",
    "    \n",
    "    if agent_data['roi']:\n",
    "        roi_error = abs(agent_data['roi'] - gt_roi) / gt_roi * 100\n",
    "    \n",
    "    result = {\n",
    "        'film': film_title_short,\n",
    "        'year': year,\n",
    "        'gt_budget': gt_budget,\n",
    "        'agent_budget': agent_data['budget'],\n",
    "        'budget_error_pct': budget_error,\n",
    "        'gt_gross': gt_gross,\n",
    "        'agent_gross': agent_data['gross'],\n",
    "        'gross_error_pct': gross_error,\n",
    "        'gt_roi': gt_roi,\n",
    "        'agent_roi': agent_data['roi'],\n",
    "        'roi_error_pct': roi_error,\n",
    "        'agent_response': agent_response[:200],\n",
    "        'citations_count': len(citations) if citations else 0\n",
    "    }\n",
    "    \n",
    "    evaluation_results.append(result)\n",
    "    \n",
    "    print(f\"  Budget: ${gt_budget/1e6:.0f}M (GT) vs ${agent_data['budget']/1e6:.0f}M (Agent) - Error: {budget_error:.1f}%\" if agent_data['budget'] else \"  Budget: Not extracted\")\n",
    "    print(f\"  Gross: ${gt_gross/1e6:.0f}M (GT) vs ${agent_data['gross']/1e6:.0f}M (Agent) - Error: {gross_error:.1f}%\" if agent_data['gross'] else \"  Gross: Not extracted\")\n",
    "    print(f\"  ROI: {gt_roi:.1f}% (GT) vs {agent_data['roi']:.1f}% (Agent) - Error: {roi_error:.1f}%\" if agent_data['roi'] else \"  ROI: Not calculated\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GROUND TRUTH EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall accuracy metrics\n",
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Data extraction success rate\n",
    "budget_extracted = eval_df['agent_budget'].notna().sum()\n",
    "gross_extracted = eval_df['agent_gross'].notna().sum()\n",
    "roi_calculated = eval_df['agent_roi'].notna().sum()\n",
    "\n",
    "print(f\"\\nData Extraction Success Rate:\")\n",
    "print(f\"  Budget extracted: {budget_extracted}/{len(eval_df)} ({budget_extracted/len(eval_df)*100:.0f}%)\")\n",
    "print(f\"  Gross extracted: {gross_extracted}/{len(eval_df)} ({gross_extracted/len(eval_df)*100:.0f}%)\")\n",
    "print(f\"  ROI calculated: {roi_calculated}/{len(eval_df)} ({roi_calculated/len(eval_df)*100:.0f}%)\")\n",
    "\n",
    "# Accuracy of extracted values (within 5% is considered accurate)\n",
    "budget_accurate = (eval_df['budget_error_pct'] < 5).sum()\n",
    "gross_accurate = (eval_df['gross_error_pct'] < 5).sum()\n",
    "roi_accurate = (eval_df['roi_error_pct'] < 5).sum()\n",
    "\n",
    "print(f\"\\nExtraction Accuracy (within 5% of ground truth):\")\n",
    "print(f\"  Budget accurate: {budget_accurate}/{budget_extracted} ({budget_accurate/budget_extracted*100:.0f}%)\" if budget_extracted > 0 else \"  Budget accurate: N/A\")\n",
    "print(f\"  Gross accurate: {gross_accurate}/{gross_extracted} ({gross_accurate/gross_extracted*100:.0f}%)\" if gross_extracted > 0 else \"  Gross accurate: N/A\")\n",
    "print(f\"  ROI accurate: {roi_accurate}/{roi_calculated} ({roi_accurate/roi_calculated*100:.0f}%)\" if roi_calculated > 0 else \"  ROI accurate: N/A\")\n",
    "\n",
    "# Average errors\n",
    "print(f\"\\nAverage Errors:\")\n",
    "print(f\"  Budget: {eval_df['budget_error_pct'].mean():.2f}%\")\n",
    "print(f\"  Gross: {eval_df['gross_error_pct'].mean():.2f}%\")\n",
    "print(f\"  ROI: {eval_df['roi_error_pct'].mean():.2f}%\")\n",
    "\n",
    "print(f\"\\nâœ¨ Average citations per response: {eval_df['citations_count'].mean():.1f}\")\n",
    "\n",
    "# Show the dataframe\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED RESULTS TABLE\")\n",
    "print(\"=\"*60)\n",
    "display(eval_df[['film', 'year', 'budget_error_pct', 'gross_error_pct', 'roi_error_pct', 'citations_count']])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
